{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'] = train['keyword'].str.replace('%20', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location'] = train['location'].str.replace('United States', 'USA', case=False)\n",
    "train['location'] = train['location'].replace('US', 'USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['location'] = train['location'].str.replace('United Kingdom', 'UK', case=False)\n",
    "train['location'] = train['location'].str.replace('California, USA', 'California', case=False)\n",
    "train['location'] = train['location'].str.replace('New York, NY', 'New York', case=False)\n",
    "train['location'] = train['location'].str.replace('New York ,NY', 'New York', case=False)\n",
    "train['location'] = train['location'].str.replace('New York, USA', 'New York', case=False)\n",
    "train['location'] = train['location'].str.replace('New York City', 'New York', case=False)\n",
    "train['location'] = train['location'].str.replace('NYC', 'New York', case=False)\n",
    "train['location'] = train['location'].str.replace('Washington, D.C.', 'Washington, DC', case=False)\n",
    "train['location'] = train['location'].str.replace('Los Angeles, CA', 'Los Angeles', case=False)\n",
    "train['location'] = train['location'].str.replace('London, UK', 'London', case=False)\n",
    "train['location'] = train['location'].replace('worldwide', 'Worldwide')\n",
    "train['location'] = train['location'].replace('Denver, Colorado', 'Devnver, CO')\n",
    "train['location'] = train['location'].replace('Seattle, WA', 'Seattle')\n",
    "train['location'] = train['location'].str.replace('Earth', 'Worldwide', case=False)\n",
    "train['location'] = train['location'].str.replace('Everywhere', 'Worldwide', case=False)\n",
    "train['location'] = train['location'].str.replace('San Francisco, CA', 'San Francisco', case=False)\n",
    "train['location'] = train['location'].str.replace('London, England', 'London', case=False)\n",
    "train['location'] = train['location'].str.replace('Chicago, IL', 'Chicago', case=False)\n",
    "train['location'] = train['location'].replace('World', 'Worldwide')\n",
    "train['location'] = train['location'].replace('M!$$!$$!PP!', 'Mississippi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = 'abcdefghijklmnopqrstuvwxyz0123456789 '\n",
    "text = {}\n",
    "len_english = len(english)\n",
    "len_train = len(train['text'])\n",
    "for i in range(0, len_train):\n",
    "    len_text = len(train['text'].iloc[i])\n",
    "    for j in range(0, len_text):\n",
    "        lower_case = train['text'].iloc[i].lower()\n",
    "        special_character = []\n",
    "        if lower_case[j] not in english:\n",
    "            if lower_case[j] in text:\n",
    "                special_character = text[lower_case[j]]\n",
    "            special_character.append(i)\n",
    "            text[lower_case[j]] = special_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample():\n",
    "    for key in text.keys():\n",
    "        sample = text[key][0]\n",
    "        print(train['text'].iloc[sample])\n",
    "        \n",
    "show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.replace('&amp', 'and')\n",
    "train['text'] = train['text'].str.replace('amp;', 'and')\n",
    "train['text'] = train['text'].str.replace('~', ' ')\n",
    "train['text'] = train['text'].str.replace('Ûª', '\\'')\n",
    "train['text'] = train['text'].str.replace('ÛÒ', ' ')\n",
    "train['text'] = train['text'].str.replace('ÛÓ', ' ')\n",
    "train['text'] = train['text'].str.replace('&gt;', ' ')\n",
    "train['text'] = train['text'].str.replace('&lt;', ' ')\n",
    "train['text'] = train['text'].str.replace('Û÷', '')\n",
    "train['text'] = train['text'].str.replace('ÛÏ', ' ')\n",
    "train['text'] = train['text'].str.replace('Û', ' ')\n",
    "train['text'] = train['text'].str.replace('\\n', ' ')\n",
    "train['text'] = train['text'].str.replace('å«', ' ')\n",
    "train['text'] = train['text'].str.replace('åÊ', ' ')\n",
    "train['text'] = train['text'].str.replace('åÇ', ' ')\n",
    "train['text'] = train['text'].str.replace('Ì©', 'e')\n",
    "train['text'] = train['text'].str.replace('Ì¤', 'c')\n",
    "train['text'] = train['text'].str.replace('Ì¼', 'u')\n",
    "train['text'] = train['text'].str.replace('Ì_', 'o')\n",
    "train['text'] = train['text'].str.replace('`', '\\'' )\n",
    "train['text'] = train['text'].str.replace('\\x89', ' ')\n",
    "train['text'] = train['text'].str.replace('\\x9d', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buscamos palabras/signos que hayan quedado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk #pip install nltk\n",
    "nltk.download('punkt')\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, smooth_idf=False, ngram_range=(1,1), stop_words='english', tokenizer=nltk.word_tokenize)\n",
    "tf_idf_data = tf_idf_vec.fit_transform(train['text'])\n",
    "col_names = tf_idf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    token_text = word_tokenize(text.lower())\n",
    "    new_text = []\n",
    "    for word in token_text:\n",
    "        word = re.sub(\"(http|https)\", \"\", word)\n",
    "        word = re.sub(\"//t\\.co/(.*)\", \"link\", word)\n",
    "        word = re.sub(\"n't\", \" not\", word)\n",
    "        word = re.sub(\"\\'s\", \" is\", word)\n",
    "        word = re.sub(\"\\'re\", \" are\", word)\n",
    "        word = re.sub(\"\\.|/|\\+|-|:\", \" \", word)\n",
    "        word = re.sub(\"( ' )|( # )\", \" \", word)\n",
    "        word = re.sub(r\"åÈ|&;|Û_|å¨|ã¢|ÌÑ|Ì¢|¤}|Ìü|Ì´|:\\\\\\\\\\\\|\\\\|[\\'©¢åâÂ¡^¬=<>\\|\\[\\]\\(\\)\\{\\}_,#;¡!¿?£$\\*¼%@¤]\", \"\", word)\n",
    "        word = re.sub(\" 0+\", \" 0 \", word)\n",
    "        new_text.append(porter.stem(word))\n",
    "        new_text.append(\" \")\n",
    "    \n",
    "    return \"\".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
