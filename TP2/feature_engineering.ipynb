{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ben/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ben/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean, median\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import category_encoders as ce #pip install category_encoders\n",
    "import nltk #pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encoder(col_name, col):\n",
    "    \n",
    "    ce_bin = ce.BinaryEncoder(cols = [col_name], drop_invariant=True)\n",
    "    \n",
    "    return ce_bin.fit_transform(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_encoder(col_text):\n",
    "    \n",
    "    tf_idf_vec = TfidfVectorizer(use_idf=True, smooth_idf=False, ngram_range=(1,1), tokenizer=nltk.word_tokenize)\n",
    "    tf_idf_data = tf_idf_vec.fit_transform(col_text)\n",
    "    tf_idf_array = tf_idf_data.toarray()\n",
    "    vocabulary = tf_idf_vec.vocabulary_\n",
    "    \n",
    "    return tf_idf_array, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_values(col, tf_idf_array, vocabulary):\n",
    "\n",
    "    len_text = len(col)\n",
    "    sum_values = []; min_values = []; max_values = []; mean_values = []; median_values = []\n",
    "    \n",
    "    for i in range(len_text):\n",
    "        values = []\n",
    "        for word in col.iloc[i].split():\n",
    "            if word in vocabulary:\n",
    "                pos_word = vocabulary[word]\n",
    "                values.append(tf_idf_array[i][pos_word])\n",
    "            \n",
    "        if values:\n",
    "            sum_values.append(sum(values))\n",
    "            min_values.append(min(values))\n",
    "            max_values.append(max(values))\n",
    "            mean_values.append(mean(values))\n",
    "            median_values.append(median(values))\n",
    "        else:\n",
    "            sum_values.append(0)\n",
    "            min_values.append(0)\n",
    "            max_values.append(0)\n",
    "            mean_values.append(0)\n",
    "            median_values.append(0)\n",
    "            \n",
    "    column_values = {'sum': sum_values, 'min': min_values, 'max': max_values,\\\n",
    "                     'mean': mean_values, 'median': median_values}\n",
    "    return column_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google_news = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_model(list_of_words):\n",
    "    model = word2vec.Word2Vec(min_count=2)\n",
    "    model.build_vocab(list_of_words)\n",
    "    model.train(list_of_words, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(x, y):\n",
    "    return np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_values_w2v(col, w2v):\n",
    "    \n",
    "    len_col = len(col)\n",
    "    sum_values = []; min_values = []; max_values = []; mean_values = []; median_values = []\n",
    "    \n",
    "    for i in range(len_col):\n",
    "        words = col.iloc[i].split()\n",
    "        array_values = []\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                array_values.append(w2v[word])\n",
    "        values = []\n",
    "        len_array_values = len(array_values)\n",
    "        for j in range(len_array_values):\n",
    "            for k in range(j+1, len_array_values):\n",
    "                values.append(cos(array_values[j], array_values[k]))\n",
    "                \n",
    "        if values:\n",
    "            sum_values.append(sum(values))\n",
    "            min_values.append(min(values))\n",
    "            max_values.append(max(values))\n",
    "            mean_values.append(mean(values))\n",
    "            median_values.append(median(values))\n",
    "        else:\n",
    "            sum_values.append(0)\n",
    "            min_values.append(0)\n",
    "            max_values.append(0)\n",
    "            mean_values.append(0)\n",
    "            median_values.append(0)\n",
    "            \n",
    "    column_values = {'sum': sum_values, 'min': min_values, 'max': max_values,\\\n",
    "                     'mean': mean_values, 'median': median_values}\n",
    "    return column_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_of_norms(col, w2v, num_n_gram=1):\n",
    "    \n",
    "    len_col = len(col)\n",
    "    sum_values = [];\n",
    "    \n",
    "    for i in range(len_col):\n",
    "        words = []\n",
    "        n_grams = ngrams(col.iloc[i].split(), num_n_gram)\n",
    "        for grams in n_grams:\n",
    "            words.append(' '.join(grams))\n",
    "        array_values = []\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                array_values.append(w2v[word])\n",
    "        values = []\n",
    "        len_array_values = len(array_values)\n",
    "        for j in range(len_array_values):\n",
    "            values.append(np.linalg.norm(array_values[j]))\n",
    "                \n",
    "        if values:\n",
    "            sum_values.append(sum(values))\n",
    "        else:\n",
    "            sum_values.append(0)\n",
    "            \n",
    "    column_values = {'sum': sum_values}\n",
    "    return column_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_norm_of_sum(col, w2v, num_n_gram=1):\n",
    "    \n",
    "    len_col = len(col)\n",
    "    norms = [];\n",
    "    \n",
    "    for i in range(len_col):\n",
    "        words = []\n",
    "        n_grams = ngrams(col.iloc[i].split(), num_n_gram)\n",
    "        for grams in n_grams:\n",
    "            words.append(' '.join(grams))\n",
    "        array_values = []\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                array_values.append(w2v[word])\n",
    "                \n",
    "        norms.append(np.linalg.norm(np.sum(array_values, axis=0)))\n",
    "                \n",
    "    return norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_len(col):\n",
    "    \n",
    "    len_string = []\n",
    "    len_col = len(col)\n",
    "    for i in range(len_col):\n",
    "        if col.iloc[i] is not np.nan:\n",
    "            len_string.append(len(col.iloc[i]))\n",
    "        else:\n",
    "            len_string.append(0)\n",
    "        \n",
    "    return len_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_list(col, num_n_gram=1):\n",
    "    len_col = len(col)\n",
    "    words = []\n",
    "    for i in range(len_col):    \n",
    "        n_grams = ngrams(col.iloc[i].split(), num_n_gram)\n",
    "        for grams in n_grams:\n",
    "            words.append([' '.join(grams)])\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train/train_limpio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(train['keyword_sin_stemming'].astype('str'), model_google_news)\n",
    "\n",
    "train['keyword_value_sin_stemming'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-365ba72d3a2d>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-365ba72d3a2d>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "keyword_as_list = get_n_gram_list(train['keyword_con_stemming'].astype('str'))\n",
    "model_keyword = get_w2v_model(keyword_as_list)\n",
    "\n",
    "column_values = calculate_sum_of_norms(train['keyword_con_stemming'].astype('str'), model_keyword)\n",
    "\n",
    "train[\"keyword_value_con_stemming\"] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword_value_mult'] = train['keyword_value_sin_stemming']*train['keyword_value_con_stemming']\n",
    "train['keyword_value_diff'] = abs(train['keyword_value_sin_stemming']-train['keyword_value_con_stemming'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(train['keyword_con_stemming'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(train['keyword_con_stemming'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "train['keyword_sum'] = [value for value in column_values['sum']]\n",
    "train['keyword_min'] = [value for value in column_values['min']]\n",
    "train['keyword_max'] = [value for value in column_values['max']]\n",
    "train['keyword_mean'] = [value for value in column_values['mean']]\n",
    "train['keyword_median'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_keyword = calculate_len(train['keyword_con_stemming'])\n",
    "\n",
    "pos_col_keyword = train.columns.get_loc('keyword_con_stemming')+1\n",
    "train.insert(loc=pos_col_keyword, column='len_keyword', value=len_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-365ba72d3a2d>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-365ba72d3a2d>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    location_as_list = get_n_gram_list(train['location'].astype('str'), i)\n",
    "    model_location = get_w2v_model(location_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(train['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(train['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_norm_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_as_list = get_n_gram_list(train['location'].astype('str'), 1)\n",
    "model_location = get_w2v_model(location_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-4793a7e59de5>:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-8-4793a7e59de5>:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "column_values = calculate_values_w2v(train['location'].astype('str'), model_location)\n",
    "\n",
    "train['location_sum_w2v'] = [value for value in column_values['sum']]\n",
    "train['location_min_w2v'] = [value for value in column_values['min']]\n",
    "train['location_max_w2v'] = [value for value in column_values['max']]\n",
    "train['location_mean_w2v'] = [value for value in column_values['mean']]\n",
    "train['location_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(train['location'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(train['location'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "train['location_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "train['location_min_tf-idf'] = [value for value in column_values['min']]\n",
    "train['location_max_tf-idf'] = [value for value in column_values['max']]\n",
    "train['location_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "train['location_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_location = calculate_len(train['location'])\n",
    "\n",
    "pos_col_location = train.columns.get_loc('location')+1\n",
    "train.insert(loc=pos_col_location, column='len_location', value=len_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(train['text_con_stemming'])\n",
    "\n",
    "column_values = calculate_values(train['text_con_stemming'], tf_idf_array, vocabulary)\n",
    "\n",
    "train['text_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "train['text_min_tf-idf'] = [value for value in column_values['min']]\n",
    "train['text_max_tf-idf'] = [value for value in column_values['max']]\n",
    "train['text_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "train['text_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(train['text_sin_stemming'], model_google_news)\n",
    "\n",
    "train['text_value'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_norm_of_sum(train['text_sin_stemming'], model_google_news)\n",
    "\n",
    "train['text_norm_value'] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_values_w2v(train['text_sin_stemming'], model_google_news)\n",
    "\n",
    "train['text_sum_w2v'] = [value for value in column_values['sum']]\n",
    "train['text_min_w2v'] = [value for value in column_values['min']]\n",
    "train['text_max_w2v'] = [value for value in column_values['max']]\n",
    "train['text_mean_w2v'] = [value for value in column_values['mean']]\n",
    "train['text_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-365ba72d3a2d>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-365ba72d3a2d>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    text_as_list = get_n_gram_list(train['text_con_stemming'].astype('str'), i)\n",
    "    model_text = get_w2v_model(text_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(train['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(train['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_norm_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_value_mult'] = train['text_value']*train['text_value_1_gram']\n",
    "train['text_value_diff'] = abs(train['text_value']-train['text_value_1_gram'])\n",
    "train['text_norm_value_mult'] = train['text_norm_value']*train['text_norm_value_1_gram']\n",
    "train['text_norm_value_diff'] = abs(train['text_norm_value']-train['text_norm_value_1_gram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>len_location</th>\n",
       "      <th>keyword_sin_stemming</th>\n",
       "      <th>keyword_con_stemming</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>text_con_stemming</th>\n",
       "      <th>text_sin_stemming</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_value_sin_stemming</th>\n",
       "      <th>keyword_value_con_stemming</th>\n",
       "      <th>...</th>\n",
       "      <th>text_value_2_gram</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "      <th>text_value_mult</th>\n",
       "      <th>text_value_diff</th>\n",
       "      <th>text_norm_value_mult</th>\n",
       "      <th>text_norm_value_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>deed reason earthquak may alah forgiv us al</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057406</td>\n",
       "      <td>0.039129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.328488</td>\n",
       "      <td>27.200895</td>\n",
       "      <td>1.034131</td>\n",
       "      <td>12.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058172</td>\n",
       "      <td>0.039828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183374</td>\n",
       "      <td>14.948923</td>\n",
       "      <td>0.482472</td>\n",
       "      <td>7.746799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>al resid ask shelter place notifi offic evacu ...</td>\n",
       "      <td>al residents asked to shelter in place are bei...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058736</td>\n",
       "      <td>0.058736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.902513</td>\n",
       "      <td>50.867533</td>\n",
       "      <td>2.458198</td>\n",
       "      <td>21.204076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>number peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>number  people receive wildfires evacuation o...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058909</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.867260</td>\n",
       "      <td>24.192476</td>\n",
       "      <td>0.804374</td>\n",
       "      <td>11.435785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.930953</td>\n",
       "      <td>38.388048</td>\n",
       "      <td>1.239480</td>\n",
       "      <td>15.629609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>two giant crane hold bridg colaps nearbi home ...</td>\n",
       "      <td>two giant cranes holding a bridge colapse into...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229595</td>\n",
       "      <td>0.081231</td>\n",
       "      <td>0.200066</td>\n",
       "      <td>0.076401</td>\n",
       "      <td>0.175942</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>6.251264</td>\n",
       "      <td>23.519079</td>\n",
       "      <td>0.907812</td>\n",
       "      <td>10.779492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>ariahrari thetawniest control wild fire califo...</td>\n",
       "      <td>ariahrary thetawniest the out of control wild ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290137</td>\n",
       "      <td>0.085929</td>\n",
       "      <td>0.255415</td>\n",
       "      <td>0.090223</td>\n",
       "      <td>0.228933</td>\n",
       "      <td>0.070228</td>\n",
       "      <td>10.658496</td>\n",
       "      <td>33.534944</td>\n",
       "      <td>1.475837</td>\n",
       "      <td>15.424894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>number number number number utc number km volc...</td>\n",
       "      <td>m number   number   number   number  utc numbe...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256325</td>\n",
       "      <td>0.112936</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.086771</td>\n",
       "      <td>0.206868</td>\n",
       "      <td>0.077030</td>\n",
       "      <td>8.823204</td>\n",
       "      <td>30.397755</td>\n",
       "      <td>2.377698</td>\n",
       "      <td>15.240896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>polic investig e bike colid car littl portug e...</td>\n",
       "      <td>police investigating after an e bike colided w...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.462007</td>\n",
       "      <td>0.111862</td>\n",
       "      <td>0.433861</td>\n",
       "      <td>0.109249</td>\n",
       "      <td>0.406871</td>\n",
       "      <td>0.112182</td>\n",
       "      <td>25.300413</td>\n",
       "      <td>51.636186</td>\n",
       "      <td>2.709726</td>\n",
       "      <td>21.319579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>latest home raze northern california wildfir a...</td>\n",
       "      <td>the latest  more homes razed by northern calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228825</td>\n",
       "      <td>0.075668</td>\n",
       "      <td>0.202807</td>\n",
       "      <td>0.081158</td>\n",
       "      <td>0.175936</td>\n",
       "      <td>0.071055</td>\n",
       "      <td>8.699998</td>\n",
       "      <td>33.251702</td>\n",
       "      <td>1.264146</td>\n",
       "      <td>14.001665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     location  len_location keyword_sin_stemming keyword_con_stemming  \\\n",
       "0         NaN             0                  NaN                  NaN   \n",
       "1         NaN             0                  NaN                  NaN   \n",
       "2         NaN             0                  NaN                  NaN   \n",
       "3         NaN             0                  NaN                  NaN   \n",
       "4         NaN             0                  NaN                  NaN   \n",
       "...       ...           ...                  ...                  ...   \n",
       "7608      NaN             0                  NaN                  NaN   \n",
       "7609      NaN             0                  NaN                  NaN   \n",
       "7610      NaN             0                  NaN                  NaN   \n",
       "7611      NaN             0                  NaN                  NaN   \n",
       "7612      NaN             0                  NaN                  NaN   \n",
       "\n",
       "      len_keyword                                  text_con_stemming  \\\n",
       "0               0        deed reason earthquak may alah forgiv us al   \n",
       "1               0               forest fire near la rong sask canada   \n",
       "2               0  al resid ask shelter place notifi offic evacu ...   \n",
       "3               0  number peopl receiv wildfir evacu order califo...   \n",
       "4               0  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "...           ...                                                ...   \n",
       "7608            0  two giant crane hold bridg colaps nearbi home ...   \n",
       "7609            0  ariahrari thetawniest control wild fire califo...   \n",
       "7610            0  number number number number utc number km volc...   \n",
       "7611            0  polic investig e bike colid car littl portug e...   \n",
       "7612            0  latest home raze northern california wildfir a...   \n",
       "\n",
       "                                      text_sin_stemming  target  \\\n",
       "0     our deeds are the reason of this earthquake ma...       1   \n",
       "1                forest fire near la ronge sask  canada       1   \n",
       "2     al residents asked to shelter in place are bei...       1   \n",
       "3      number  people receive wildfires evacuation o...       1   \n",
       "4     just got sent this photo from ruby alaska as s...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  two giant cranes holding a bridge colapse into...       1   \n",
       "7609  ariahrary thetawniest the out of control wild ...       1   \n",
       "7610  m number   number   number   number  utc numbe...       1   \n",
       "7611  police investigating after an e bike colided w...       1   \n",
       "7612  the latest  more homes razed by northern calif...       1   \n",
       "\n",
       "      keyword_value_sin_stemming  keyword_value_con_stemming  ...  \\\n",
       "0                       3.267254                     0.03008  ...   \n",
       "1                       3.267254                     0.03008  ...   \n",
       "2                       3.267254                     0.03008  ...   \n",
       "3                       3.267254                     0.03008  ...   \n",
       "4                       3.267254                     0.03008  ...   \n",
       "...                          ...                         ...  ...   \n",
       "7608                    3.267254                     0.03008  ...   \n",
       "7609                    3.267254                     0.03008  ...   \n",
       "7610                    3.267254                     0.03008  ...   \n",
       "7611                    3.267254                     0.03008  ...   \n",
       "7612                    3.267254                     0.03008  ...   \n",
       "\n",
       "      text_value_2_gram  text_norm_value_2_gram  text_value_3_gram  \\\n",
       "0              0.057406                0.039129           0.000000   \n",
       "1              0.058172                0.039828           0.000000   \n",
       "2              0.058736                0.058736           0.000000   \n",
       "3              0.058909                0.040829           0.000000   \n",
       "4              0.000000                0.000000           0.000000   \n",
       "...                 ...                     ...                ...   \n",
       "7608           0.229595                0.081231           0.200066   \n",
       "7609           0.290137                0.085929           0.255415   \n",
       "7610           0.256325                0.112936           0.238182   \n",
       "7611           0.462007                0.111862           0.433861   \n",
       "7612           0.228825                0.075668           0.202807   \n",
       "\n",
       "      text_norm_value_3_gram  text_value_4_gram  text_norm_value_4_gram  \\\n",
       "0                   0.000000           0.000000                0.000000   \n",
       "1                   0.000000           0.000000                0.000000   \n",
       "2                   0.000000           0.000000                0.000000   \n",
       "3                   0.000000           0.000000                0.000000   \n",
       "4                   0.000000           0.000000                0.000000   \n",
       "...                      ...                ...                     ...   \n",
       "7608                0.076401           0.175942                0.074368   \n",
       "7609                0.090223           0.228933                0.070228   \n",
       "7610                0.086771           0.206868                0.077030   \n",
       "7611                0.109249           0.406871                0.112182   \n",
       "7612                0.081158           0.175936                0.071055   \n",
       "\n",
       "      text_value_mult  text_value_diff  text_norm_value_mult  \\\n",
       "0            6.328488        27.200895              1.034131   \n",
       "1            2.183374        14.948923              0.482472   \n",
       "2           15.902513        50.867533              2.458198   \n",
       "3            4.867260        24.192476              0.804374   \n",
       "4            9.930953        38.388048              1.239480   \n",
       "...               ...              ...                   ...   \n",
       "7608         6.251264        23.519079              0.907812   \n",
       "7609        10.658496        33.534944              1.475837   \n",
       "7610         8.823204        30.397755              2.377698   \n",
       "7611        25.300413        51.636186              2.709726   \n",
       "7612         8.699998        33.251702              1.264146   \n",
       "\n",
       "      text_norm_value_diff  \n",
       "0                12.431300  \n",
       "1                 7.746799  \n",
       "2                21.204076  \n",
       "3                11.435785  \n",
       "4                15.629609  \n",
       "...                    ...  \n",
       "7608             10.779492  \n",
       "7609             15.424894  \n",
       "7610             15.240896  \n",
       "7611             21.319579  \n",
       "7612             14.001665  \n",
       "\n",
       "[7613 rows x 57 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removemos target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "cols.append('target')\n",
    "train = train[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>len_location</th>\n",
       "      <th>keyword_sin_stemming</th>\n",
       "      <th>keyword_con_stemming</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>text_con_stemming</th>\n",
       "      <th>len_text</th>\n",
       "      <th>text_sin_stemming</th>\n",
       "      <th>keyword_value_sin_stemming</th>\n",
       "      <th>keyword_value_con_stemming</th>\n",
       "      <th>...</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "      <th>text_value_mult</th>\n",
       "      <th>text_value_diff</th>\n",
       "      <th>text_norm_value_mult</th>\n",
       "      <th>text_norm_value_diff</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>deed reason earthquak may alah forgiv us al</td>\n",
       "      <td>43</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.328488</td>\n",
       "      <td>27.200895</td>\n",
       "      <td>1.034131</td>\n",
       "      <td>12.431300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>36</td>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039828</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.183374</td>\n",
       "      <td>14.948923</td>\n",
       "      <td>0.482472</td>\n",
       "      <td>7.746799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>al resid ask shelter place notifi offic evacu ...</td>\n",
       "      <td>72</td>\n",
       "      <td>al residents asked to shelter in place are bei...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.902513</td>\n",
       "      <td>50.867533</td>\n",
       "      <td>2.458198</td>\n",
       "      <td>21.204076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>number peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>50</td>\n",
       "      <td>number  people receive wildfires evacuation o...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.867260</td>\n",
       "      <td>24.192476</td>\n",
       "      <td>0.804374</td>\n",
       "      <td>11.435785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>52</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.930953</td>\n",
       "      <td>38.388048</td>\n",
       "      <td>1.239480</td>\n",
       "      <td>15.629609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>rockyfir updat california hwi number close dir...</td>\n",
       "      <td>84</td>\n",
       "      <td>rockyfire update  california hwy   number  clo...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.091701</td>\n",
       "      <td>36.979880</td>\n",
       "      <td>1.732342</td>\n",
       "      <td>15.621549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>flood disast heavi rain caus flash flood stree...</td>\n",
       "      <td>74</td>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.891867</td>\n",
       "      <td>34.133985</td>\n",
       "      <td>1.789968</td>\n",
       "      <td>16.191501</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>top hil see fire wood</td>\n",
       "      <td>21</td>\n",
       "      <td>i am on top of the hil and i can see a fire in...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.093972</td>\n",
       "      <td>26.802238</td>\n",
       "      <td>0.806041</td>\n",
       "      <td>12.256102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>emerg evacu happen build across street</td>\n",
       "      <td>38</td>\n",
       "      <td>there is an emergency evacuation happening now...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.706527</td>\n",
       "      <td>27.632688</td>\n",
       "      <td>0.961200</td>\n",
       "      <td>13.417604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>afraid tornado come area</td>\n",
       "      <td>24</td>\n",
       "      <td>i am afraid that the tornado is coming to our ...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.697300</td>\n",
       "      <td>22.948983</td>\n",
       "      <td>0.702365</td>\n",
       "      <td>11.044126</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  location  len_location keyword_sin_stemming keyword_con_stemming  \\\n",
       "0      NaN             0                  NaN                  NaN   \n",
       "1      NaN             0                  NaN                  NaN   \n",
       "2      NaN             0                  NaN                  NaN   \n",
       "3      NaN             0                  NaN                  NaN   \n",
       "4      NaN             0                  NaN                  NaN   \n",
       "5      NaN             0                  NaN                  NaN   \n",
       "6      NaN             0                  NaN                  NaN   \n",
       "7      NaN             0                  NaN                  NaN   \n",
       "8      NaN             0                  NaN                  NaN   \n",
       "9      NaN             0                  NaN                  NaN   \n",
       "\n",
       "   len_keyword                                  text_con_stemming  len_text  \\\n",
       "0            0        deed reason earthquak may alah forgiv us al        43   \n",
       "1            0               forest fire near la rong sask canada        36   \n",
       "2            0  al resid ask shelter place notifi offic evacu ...        72   \n",
       "3            0  number peopl receiv wildfir evacu order califo...        50   \n",
       "4            0  got sent photo rubi alaska smoke wildfir pour ...        52   \n",
       "5            0  rockyfir updat california hwi number close dir...        84   \n",
       "6            0  flood disast heavi rain caus flash flood stree...        74   \n",
       "7            0                              top hil see fire wood        21   \n",
       "8            0             emerg evacu happen build across street        38   \n",
       "9            0                           afraid tornado come area        24   \n",
       "\n",
       "                                   text_sin_stemming  \\\n",
       "0  our deeds are the reason of this earthquake ma...   \n",
       "1             forest fire near la ronge sask  canada   \n",
       "2  al residents asked to shelter in place are bei...   \n",
       "3   number  people receive wildfires evacuation o...   \n",
       "4  just got sent this photo from ruby alaska as s...   \n",
       "5  rockyfire update  california hwy   number  clo...   \n",
       "6  flood disaster heavy rain causes flash floodin...   \n",
       "7  i am on top of the hil and i can see a fire in...   \n",
       "8  there is an emergency evacuation happening now...   \n",
       "9  i am afraid that the tornado is coming to our ...   \n",
       "\n",
       "   keyword_value_sin_stemming  keyword_value_con_stemming  ...  \\\n",
       "0                    3.267254                     0.03008  ...   \n",
       "1                    3.267254                     0.03008  ...   \n",
       "2                    3.267254                     0.03008  ...   \n",
       "3                    3.267254                     0.03008  ...   \n",
       "4                    3.267254                     0.03008  ...   \n",
       "5                    3.267254                     0.03008  ...   \n",
       "6                    3.267254                     0.03008  ...   \n",
       "7                    3.267254                     0.03008  ...   \n",
       "8                    3.267254                     0.03008  ...   \n",
       "9                    3.267254                     0.03008  ...   \n",
       "\n",
       "   text_norm_value_2_gram  text_value_3_gram  text_norm_value_3_gram  \\\n",
       "0                0.039129                0.0                     0.0   \n",
       "1                0.039828                0.0                     0.0   \n",
       "2                0.058736                0.0                     0.0   \n",
       "3                0.040829                0.0                     0.0   \n",
       "4                0.000000                0.0                     0.0   \n",
       "5                0.048779                0.0                     0.0   \n",
       "6                0.056748                0.0                     0.0   \n",
       "7                0.030275                0.0                     0.0   \n",
       "8                0.028947                0.0                     0.0   \n",
       "9                0.000000                0.0                     0.0   \n",
       "\n",
       "   text_value_4_gram  text_norm_value_4_gram  text_value_mult  \\\n",
       "0                0.0                     0.0         6.328488   \n",
       "1                0.0                     0.0         2.183374   \n",
       "2                0.0                     0.0        15.902513   \n",
       "3                0.0                     0.0         4.867260   \n",
       "4                0.0                     0.0         9.930953   \n",
       "5                0.0                     0.0        14.091701   \n",
       "6                0.0                     0.0        10.891867   \n",
       "7                0.0                     0.0         4.093972   \n",
       "8                0.0                     0.0         4.706527   \n",
       "9                0.0                     0.0         2.697300   \n",
       "\n",
       "   text_value_diff  text_norm_value_mult  text_norm_value_diff  target  \n",
       "0        27.200895              1.034131             12.431300       1  \n",
       "1        14.948923              0.482472              7.746799       1  \n",
       "2        50.867533              2.458198             21.204076       1  \n",
       "3        24.192476              0.804374             11.435785       1  \n",
       "4        38.388048              1.239480             15.629609       1  \n",
       "5        36.979880              1.732342             15.621549       1  \n",
       "6        34.133985              1.789968             16.191501       1  \n",
       "7        26.802238              0.806041             12.256102       1  \n",
       "8        27.632688              0.961200             13.417604       1  \n",
       "9        22.948983              0.702365             11.044126       1  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(train['text_con_stemming'])\n",
    "\n",
    "pos_col_text = train.columns.get_loc('text_con_stemming')+1\n",
    "train.insert(loc=pos_col_text, column='len_text', value=len_text)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_original = pd.read_csv('train/train_original.csv')\n",
    "train_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>len_text_original</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   len_text_original  target  \n",
       "0                 69       1  \n",
       "1                 38       1  \n",
       "2                133       1  \n",
       "3                 65       1  \n",
       "4                 88       1  \n",
       "5                110       1  \n",
       "6                 95       1  \n",
       "7                 59       1  \n",
       "8                 79       1  \n",
       "9                 52       1  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(train_original['text'])\n",
    "\n",
    "pos_col_text = train_original.columns.get_loc('text')+1\n",
    "train_original.insert(loc=pos_col_text, column='len_text_original', value=len_text)\n",
    "train_original.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns.tolist()\n",
    "cols.remove('location')\n",
    "cols.remove('text_con_stemming')\n",
    "cols.remove('text_sin_stemming')\n",
    "cols.remove('keyword_con_stemming')\n",
    "cols.remove('keyword_sin_stemming')\n",
    "cols.remove('target')\n",
    "cols.append('target')\n",
    "train = train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = train[cols]\n",
    "\n",
    "pos_col_text = train_encoded.columns.get_loc('len_text')+1\n",
    "train_encoded.insert(loc=pos_col_text, column='len_text_original', value=train_original['len_text_original'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.insert(loc=pos_col_text+1, column='diff_len_text',\\\n",
    "                     value=abs(train_encoded['len_text_original']-train_encoded['len_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_location</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_text_original</th>\n",
       "      <th>diff_len_text</th>\n",
       "      <th>keyword_value_sin_stemming</th>\n",
       "      <th>keyword_value_con_stemming</th>\n",
       "      <th>keyword_value_mult</th>\n",
       "      <th>keyword_value_diff</th>\n",
       "      <th>keyword_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "      <th>text_value_mult</th>\n",
       "      <th>text_value_diff</th>\n",
       "      <th>text_norm_value_mult</th>\n",
       "      <th>text_norm_value_diff</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>69</td>\n",
       "      <td>26</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.328488</td>\n",
       "      <td>27.200895</td>\n",
       "      <td>1.034131</td>\n",
       "      <td>12.431300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.183374</td>\n",
       "      <td>14.948923</td>\n",
       "      <td>0.482472</td>\n",
       "      <td>7.746799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.902513</td>\n",
       "      <td>50.867533</td>\n",
       "      <td>2.458198</td>\n",
       "      <td>21.204076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.867260</td>\n",
       "      <td>24.192476</td>\n",
       "      <td>0.804374</td>\n",
       "      <td>11.435785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>88</td>\n",
       "      <td>36</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.930953</td>\n",
       "      <td>38.388048</td>\n",
       "      <td>1.239480</td>\n",
       "      <td>15.629609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>83</td>\n",
       "      <td>33</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081231</td>\n",
       "      <td>0.200066</td>\n",
       "      <td>0.076401</td>\n",
       "      <td>0.175942</td>\n",
       "      <td>0.074368</td>\n",
       "      <td>6.251264</td>\n",
       "      <td>23.519079</td>\n",
       "      <td>0.907812</td>\n",
       "      <td>10.779492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>125</td>\n",
       "      <td>43</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085929</td>\n",
       "      <td>0.255415</td>\n",
       "      <td>0.090223</td>\n",
       "      <td>0.228933</td>\n",
       "      <td>0.070228</td>\n",
       "      <td>10.658496</td>\n",
       "      <td>33.534944</td>\n",
       "      <td>1.475837</td>\n",
       "      <td>15.424894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112936</td>\n",
       "      <td>0.238182</td>\n",
       "      <td>0.086771</td>\n",
       "      <td>0.206868</td>\n",
       "      <td>0.077030</td>\n",
       "      <td>8.823204</td>\n",
       "      <td>30.397755</td>\n",
       "      <td>2.377698</td>\n",
       "      <td>15.240896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>137</td>\n",
       "      <td>41</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111862</td>\n",
       "      <td>0.433861</td>\n",
       "      <td>0.109249</td>\n",
       "      <td>0.406871</td>\n",
       "      <td>0.112182</td>\n",
       "      <td>25.300413</td>\n",
       "      <td>51.636186</td>\n",
       "      <td>2.709726</td>\n",
       "      <td>21.319579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>94</td>\n",
       "      <td>36</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075668</td>\n",
       "      <td>0.202807</td>\n",
       "      <td>0.081158</td>\n",
       "      <td>0.175936</td>\n",
       "      <td>0.071055</td>\n",
       "      <td>8.699998</td>\n",
       "      <td>33.251702</td>\n",
       "      <td>1.264146</td>\n",
       "      <td>14.001665</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      len_location  len_keyword  len_text  len_text_original  diff_len_text  \\\n",
       "0                0            0        43                 69             26   \n",
       "1                0            0        36                 38              2   \n",
       "2                0            0        72                133             61   \n",
       "3                0            0        50                 65             15   \n",
       "4                0            0        52                 88             36   \n",
       "...            ...          ...       ...                ...            ...   \n",
       "7608             0            0        50                 83             33   \n",
       "7609             0            0        82                125             43   \n",
       "7610             0            0        60                 65              5   \n",
       "7611             0            0        96                137             41   \n",
       "7612             0            0        58                 94             36   \n",
       "\n",
       "      keyword_value_sin_stemming  keyword_value_con_stemming  \\\n",
       "0                       3.267254                     0.03008   \n",
       "1                       3.267254                     0.03008   \n",
       "2                       3.267254                     0.03008   \n",
       "3                       3.267254                     0.03008   \n",
       "4                       3.267254                     0.03008   \n",
       "...                          ...                         ...   \n",
       "7608                    3.267254                     0.03008   \n",
       "7609                    3.267254                     0.03008   \n",
       "7610                    3.267254                     0.03008   \n",
       "7611                    3.267254                     0.03008   \n",
       "7612                    3.267254                     0.03008   \n",
       "\n",
       "      keyword_value_mult  keyword_value_diff  keyword_sum  ...  \\\n",
       "0               0.098278            3.237175          1.0  ...   \n",
       "1               0.098278            3.237175          1.0  ...   \n",
       "2               0.098278            3.237175          1.0  ...   \n",
       "3               0.098278            3.237175          1.0  ...   \n",
       "4               0.098278            3.237175          1.0  ...   \n",
       "...                  ...                 ...          ...  ...   \n",
       "7608            0.098278            3.237175          1.0  ...   \n",
       "7609            0.098278            3.237175          1.0  ...   \n",
       "7610            0.098278            3.237175          1.0  ...   \n",
       "7611            0.098278            3.237175          1.0  ...   \n",
       "7612            0.098278            3.237175          1.0  ...   \n",
       "\n",
       "      text_norm_value_2_gram  text_value_3_gram  text_norm_value_3_gram  \\\n",
       "0                   0.039129           0.000000                0.000000   \n",
       "1                   0.039828           0.000000                0.000000   \n",
       "2                   0.058736           0.000000                0.000000   \n",
       "3                   0.040829           0.000000                0.000000   \n",
       "4                   0.000000           0.000000                0.000000   \n",
       "...                      ...                ...                     ...   \n",
       "7608                0.081231           0.200066                0.076401   \n",
       "7609                0.085929           0.255415                0.090223   \n",
       "7610                0.112936           0.238182                0.086771   \n",
       "7611                0.111862           0.433861                0.109249   \n",
       "7612                0.075668           0.202807                0.081158   \n",
       "\n",
       "      text_value_4_gram  text_norm_value_4_gram  text_value_mult  \\\n",
       "0              0.000000                0.000000         6.328488   \n",
       "1              0.000000                0.000000         2.183374   \n",
       "2              0.000000                0.000000        15.902513   \n",
       "3              0.000000                0.000000         4.867260   \n",
       "4              0.000000                0.000000         9.930953   \n",
       "...                 ...                     ...              ...   \n",
       "7608           0.175942                0.074368         6.251264   \n",
       "7609           0.228933                0.070228        10.658496   \n",
       "7610           0.206868                0.077030         8.823204   \n",
       "7611           0.406871                0.112182        25.300413   \n",
       "7612           0.175936                0.071055         8.699998   \n",
       "\n",
       "      text_value_diff  text_norm_value_mult  text_norm_value_diff  target  \n",
       "0           27.200895              1.034131             12.431300       1  \n",
       "1           14.948923              0.482472              7.746799       1  \n",
       "2           50.867533              2.458198             21.204076       1  \n",
       "3           24.192476              0.804374             11.435785       1  \n",
       "4           38.388048              1.239480             15.629609       1  \n",
       "...               ...                   ...                   ...     ...  \n",
       "7608        23.519079              0.907812             10.779492       1  \n",
       "7609        33.534944              1.475837             15.424894       1  \n",
       "7610        30.397755              2.377698             15.240896       1  \n",
       "7611        51.636186              2.709726             21.319579       1  \n",
       "7612        33.251702              1.264146             14.001665       1  \n",
       "\n",
       "[7613 rows x 55 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.to_csv('train/train_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test/test_limpio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(test['keyword_sin_stemming'].astype('str'), model_google_news)\n",
    "\n",
    "test['keyword_value_sin_stemming'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-365ba72d3a2d>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-365ba72d3a2d>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "keyword_as_list = get_n_gram_list(test['keyword_con_stemming'].astype('str'), 1)\n",
    "model_keyword = get_w2v_model(keyword_as_list)\n",
    "\n",
    "column_values = calculate_sum_of_norms(test['keyword_con_stemming'].astype('str'), model_keyword, 1)\n",
    "\n",
    "test[\"keyword_value_con_stemming\"] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['keyword_value_mult'] = test['keyword_value_sin_stemming']*test['keyword_value_con_stemming']\n",
    "test['keyword_value_diff'] = abs(test['keyword_value_sin_stemming']-test['keyword_value_con_stemming'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(test['keyword_con_stemming'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(test['keyword_con_stemming'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "test['keyword_sum'] = [value for value in column_values['sum']]\n",
    "test['keyword_min'] = [value for value in column_values['min']]\n",
    "test['keyword_max'] = [value for value in column_values['max']]\n",
    "test['keyword_mean'] = [value for value in column_values['mean']]\n",
    "test['keyword_median'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_keyword = calculate_len(test['keyword_con_stemming'])\n",
    "\n",
    "pos_col_keyword = test.columns.get_loc('keyword_con_stemming')+1\n",
    "test.insert(loc=pos_col_keyword, column='len_keyword', value=len_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-365ba72d3a2d>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-365ba72d3a2d>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    location_as_list = get_n_gram_list(test['location'].astype('str'), i)\n",
    "    model_location = get_w2v_model(location_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(test['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(test['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_norm_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-4793a7e59de5>:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n"
     ]
    }
   ],
   "source": [
    "column_values = calculate_values_w2v(test['location'].astype('str'), model_location)\n",
    "\n",
    "test['location_sum_w2v'] = [value for value in column_values['sum']]\n",
    "test['location_min_w2v'] = [value for value in column_values['min']]\n",
    "test['location_max_w2v'] = [value for value in column_values['max']]\n",
    "test['location_mean_w2v'] = [value for value in column_values['mean']]\n",
    "test['location_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(test['location'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(test['location'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "test['location_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "test['location_min_tf-idf'] = [value for value in column_values['min']]\n",
    "test['location_max_tf-idf'] = [value for value in column_values['max']]\n",
    "test['location_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "test['location_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_location = calculate_len(test['location'])\n",
    "\n",
    "pos_col_location = test.columns.get_loc('location')+1\n",
    "test.insert(loc=pos_col_location, column='len_location', value=len_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text_con_stemming'] = test['text_con_stemming'].astype('str')\n",
    "test['text_sin_stemming'] = test['text_sin_stemming'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(test['text_con_stemming'])\n",
    "\n",
    "column_values = calculate_values(test['text_con_stemming'], tf_idf_array, vocabulary)\n",
    "\n",
    "test['text_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "test['text_min_tf-idf'] = [value for value in column_values['min']]\n",
    "test['text_max_tf-idf'] = [value for value in column_values['max']]\n",
    "test['text_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "test['text_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(test['text_sin_stemming'], model_google_news)\n",
    "\n",
    "test['text_value'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_norm_of_sum(test['text_sin_stemming'], model_google_news)\n",
    "\n",
    "test['text_norm_value'] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_values_w2v(test['text_sin_stemming'], model_google_news)\n",
    "\n",
    "test['text_sum_w2v'] = [value for value in column_values['sum']]\n",
    "test['text_min_w2v'] = [value for value in column_values['min']]\n",
    "test['text_max_w2v'] = [value for value in column_values['max']]\n",
    "test['text_mean_w2v'] = [value for value in column_values['mean']]\n",
    "test['text_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-365ba72d3a2d>:13: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-365ba72d3a2d>:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    text_as_list = get_n_gram_list(test['text_con_stemming'].astype('str'), i)\n",
    "    model_text = get_w2v_model(text_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(test['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(test['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_norm_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text_value_mult'] = test['text_value']*test['text_value_1_gram']\n",
    "test['text_value_diff'] = abs(test['text_value']-test['text_value_1_gram'])\n",
    "test['text_norm_value_mult'] = test['text_norm_value']*test['text_norm_value_1_gram']\n",
    "test['text_norm_value_diff'] = abs(test['text_norm_value']-test['text_norm_value_1_gram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>len_location</th>\n",
       "      <th>keyword_sin_stemming</th>\n",
       "      <th>keyword_con_stemming</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>text_con_stemming</th>\n",
       "      <th>len_text</th>\n",
       "      <th>text_sin_stemming</th>\n",
       "      <th>keyword_value_sin_stemming</th>\n",
       "      <th>...</th>\n",
       "      <th>text_value_2_gram</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "      <th>text_value_mult</th>\n",
       "      <th>text_value_diff</th>\n",
       "      <th>text_norm_value_mult</th>\n",
       "      <th>text_norm_value_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>happen teribl car crash</td>\n",
       "      <td>23</td>\n",
       "      <td>just happened a terible car crash</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.305677</td>\n",
       "      <td>11.145168</td>\n",
       "      <td>0.433205</td>\n",
       "      <td>7.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "      <td>45</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060072</td>\n",
       "      <td>0.046347</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.531304</td>\n",
       "      <td>23.119374</td>\n",
       "      <td>0.751209</td>\n",
       "      <td>10.944429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "      <td>53</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054311</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.661704</td>\n",
       "      <td>45.098771</td>\n",
       "      <td>1.461376</td>\n",
       "      <td>17.442445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "      <td>30</td>\n",
       "      <td>apocalypse lighting  spokane wildfires</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.961869</td>\n",
       "      <td>11.282421</td>\n",
       "      <td>0.328020</td>\n",
       "      <td>7.421708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>typhoon soudelor kil number china taiwan</td>\n",
       "      <td>40</td>\n",
       "      <td>typhoon soudelor kils  number  in china and ta...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086690</td>\n",
       "      <td>0.052940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.802071</td>\n",
       "      <td>15.976074</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>8.324345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>shake earthquak</td>\n",
       "      <td>15</td>\n",
       "      <td>we are shaking   it is an earthquake</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474218</td>\n",
       "      <td>17.046467</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>8.792170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>would probabl stil show life arsen yesterday e...</td>\n",
       "      <td>50</td>\n",
       "      <td>they would probably stil show more life than a...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.704609</td>\n",
       "      <td>29.444210</td>\n",
       "      <td>1.231310</td>\n",
       "      <td>14.676575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>hey</td>\n",
       "      <td>3</td>\n",
       "      <td>hey how are you</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278601</td>\n",
       "      <td>9.277050</td>\n",
       "      <td>0.185952</td>\n",
       "      <td>6.182016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>nice hat</td>\n",
       "      <td>8</td>\n",
       "      <td>what a nice hat</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.441108</td>\n",
       "      <td>7.601308</td>\n",
       "      <td>0.199966</td>\n",
       "      <td>5.171657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>fuck</td>\n",
       "      <td>4</td>\n",
       "      <td>fuck off</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160437</td>\n",
       "      <td>5.178335</td>\n",
       "      <td>0.124176</td>\n",
       "      <td>4.000983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id location  len_location keyword_sin_stemming keyword_con_stemming  \\\n",
       "0   0      NaN             0                  NaN                  NaN   \n",
       "1   2      NaN             0                  NaN                  NaN   \n",
       "2   3      NaN             0                  NaN                  NaN   \n",
       "3   9      NaN             0                  NaN                  NaN   \n",
       "4  11      NaN             0                  NaN                  NaN   \n",
       "5  12      NaN             0                  NaN                  NaN   \n",
       "6  21      NaN             0                  NaN                  NaN   \n",
       "7  22      NaN             0                  NaN                  NaN   \n",
       "8  27      NaN             0                  NaN                  NaN   \n",
       "9  29      NaN             0                  NaN                  NaN   \n",
       "\n",
       "   len_keyword                                  text_con_stemming  len_text  \\\n",
       "0            0                            happen teribl car crash        23   \n",
       "1            0      heard earthquak differ citi stay safe everyon        45   \n",
       "2            0  forest fire spot pond gees flee across street ...        53   \n",
       "3            0                     apocalyps light spokan wildfir        30   \n",
       "4            0           typhoon soudelor kil number china taiwan        40   \n",
       "5            0                                    shake earthquak        15   \n",
       "6            0  would probabl stil show life arsen yesterday e...        50   \n",
       "7            0                                                hey         3   \n",
       "8            0                                           nice hat         8   \n",
       "9            0                                               fuck         4   \n",
       "\n",
       "                                   text_sin_stemming  \\\n",
       "0                  just happened a terible car crash   \n",
       "1  heard about earthquake is different cities sta...   \n",
       "2  there is a forest fire at spot pond geese are ...   \n",
       "3             apocalypse lighting  spokane wildfires   \n",
       "4  typhoon soudelor kils  number  in china and ta...   \n",
       "5               we are shaking   it is an earthquake   \n",
       "6  they would probably stil show more life than a...   \n",
       "7                                    hey how are you   \n",
       "8                                    what a nice hat   \n",
       "9                                           fuck off   \n",
       "\n",
       "   keyword_value_sin_stemming  ...  text_value_2_gram  text_norm_value_2_gram  \\\n",
       "0                    3.267254  ...           0.027890                0.027890   \n",
       "1                    3.267254  ...           0.060072                0.046347   \n",
       "2                    3.267254  ...           0.054311                0.037508   \n",
       "3                    3.267254  ...           0.000000                0.000000   \n",
       "4                    3.267254  ...           0.086690                0.052940   \n",
       "5                    3.267254  ...           0.000000                0.000000   \n",
       "6                    3.267254  ...           0.029976                0.029976   \n",
       "7                    3.267254  ...           0.000000                0.000000   \n",
       "8                    3.267254  ...           0.000000                0.000000   \n",
       "9                    3.267254  ...           0.000000                0.000000   \n",
       "\n",
       "   text_value_3_gram  text_norm_value_3_gram  text_value_4_gram  \\\n",
       "0           0.000000                0.000000                0.0   \n",
       "1           0.027337                0.027337                0.0   \n",
       "2           0.000000                0.000000                0.0   \n",
       "3           0.000000                0.000000                0.0   \n",
       "4           0.000000                0.000000                0.0   \n",
       "5           0.000000                0.000000                0.0   \n",
       "6           0.000000                0.000000                0.0   \n",
       "7           0.000000                0.000000                0.0   \n",
       "8           0.000000                0.000000                0.0   \n",
       "9           0.000000                0.000000                0.0   \n",
       "\n",
       "   text_norm_value_4_gram  text_value_mult  text_value_diff  \\\n",
       "0                     0.0         1.305677        11.145168   \n",
       "1                     0.0         4.531304        23.119374   \n",
       "2                     0.0        10.661704        45.098771   \n",
       "3                     0.0         0.961869        11.282421   \n",
       "4                     0.0         2.802071        15.976074   \n",
       "5                     0.0         0.474218        17.046467   \n",
       "6                     0.0         7.704609        29.444210   \n",
       "7                     0.0         0.278601         9.277050   \n",
       "8                     0.0         0.441108         7.601308   \n",
       "9                     0.0         0.160437         5.178335   \n",
       "\n",
       "   text_norm_value_mult  text_norm_value_diff  \n",
       "0              0.433205              7.178612  \n",
       "1              0.751209             10.944429  \n",
       "2              1.461376             17.442445  \n",
       "3              0.328020              7.421708  \n",
       "4              0.593750              8.324345  \n",
       "5              0.244964              8.792170  \n",
       "6              1.231310             14.676575  \n",
       "7              0.185952              6.182016  \n",
       "8              0.199966              5.171657  \n",
       "9              0.124176              4.000983  \n",
       "\n",
       "[10 rows x 58 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(test['text_con_stemming'])\n",
    "\n",
    "pos_col_text = test.columns.get_loc('text_con_stemming')+1\n",
    "test.insert(loc=pos_col_text, column='len_text', value=len_text)\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_original = pd.read_csv('test/test_original.csv')\n",
    "test_original.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test.columns.tolist()\n",
    "cols.remove('location')\n",
    "cols.remove('text_con_stemming')\n",
    "cols.remove('text_sin_stemming')\n",
    "cols.remove('keyword_con_stemming')\n",
    "cols.remove('keyword_sin_stemming')\n",
    "test_encoded = test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>len_text_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake   \n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...   \n",
       "7  22     NaN      NaN                                  Hey! How are you?   \n",
       "8  27     NaN      NaN                                   What a nice hat?   \n",
       "9  29     NaN      NaN                                          Fuck off!   \n",
       "\n",
       "   len_text_original  \n",
       "0                 34  \n",
       "1                 64  \n",
       "2                 96  \n",
       "3                 40  \n",
       "4                 45  \n",
       "5                 34  \n",
       "6                 72  \n",
       "7                 17  \n",
       "8                 16  \n",
       "9                  9  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(test_original['text'])\n",
    "\n",
    "pos_col_text = test_original.columns.get_loc('text')+1\n",
    "test_original.insert(loc=pos_col_text, column='len_text_original', value=len_text)\n",
    "test_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_col_text = test_encoded.columns.get_loc('len_text')+1\n",
    "test_encoded.insert(loc=pos_col_text, column='len_text_original', value=test_original['len_text_original'])\n",
    "\n",
    "test_encoded.insert(loc=pos_col_text+1, column='diff_len_text',\\\n",
    "                     value=abs(test_encoded['len_text_original']-test_encoded['len_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>len_location</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_text_original</th>\n",
       "      <th>diff_len_text</th>\n",
       "      <th>keyword_value_sin_stemming</th>\n",
       "      <th>keyword_value_con_stemming</th>\n",
       "      <th>keyword_value_mult</th>\n",
       "      <th>keyword_value_diff</th>\n",
       "      <th>...</th>\n",
       "      <th>text_value_2_gram</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "      <th>text_value_mult</th>\n",
       "      <th>text_value_diff</th>\n",
       "      <th>text_norm_value_mult</th>\n",
       "      <th>text_norm_value_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>0.027890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.305677</td>\n",
       "      <td>11.145168</td>\n",
       "      <td>0.433205</td>\n",
       "      <td>7.178612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060072</td>\n",
       "      <td>0.046347</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>0.027337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.531304</td>\n",
       "      <td>23.119374</td>\n",
       "      <td>0.751209</td>\n",
       "      <td>10.944429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>96</td>\n",
       "      <td>43</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054311</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.661704</td>\n",
       "      <td>45.098771</td>\n",
       "      <td>1.461376</td>\n",
       "      <td>17.442445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961869</td>\n",
       "      <td>11.282421</td>\n",
       "      <td>0.328020</td>\n",
       "      <td>7.421708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086690</td>\n",
       "      <td>0.052940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.802071</td>\n",
       "      <td>15.976074</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>8.324345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>0.026343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.791938</td>\n",
       "      <td>19.243501</td>\n",
       "      <td>0.832196</td>\n",
       "      <td>10.592598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>139</td>\n",
       "      <td>25</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142852</td>\n",
       "      <td>0.066315</td>\n",
       "      <td>0.030143</td>\n",
       "      <td>0.030143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.175360</td>\n",
       "      <td>59.897514</td>\n",
       "      <td>2.622387</td>\n",
       "      <td>23.467875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059477</td>\n",
       "      <td>0.039728</td>\n",
       "      <td>0.030895</td>\n",
       "      <td>0.030895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.446438</td>\n",
       "      <td>16.745660</td>\n",
       "      <td>0.504244</td>\n",
       "      <td>8.611057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>0.070943</td>\n",
       "      <td>0.112221</td>\n",
       "      <td>0.059870</td>\n",
       "      <td>0.086311</td>\n",
       "      <td>0.045653</td>\n",
       "      <td>4.107835</td>\n",
       "      <td>20.084189</td>\n",
       "      <td>0.686354</td>\n",
       "      <td>9.431351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>68</td>\n",
       "      <td>22</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>0.03008</td>\n",
       "      <td>0.098278</td>\n",
       "      <td>3.237175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057778</td>\n",
       "      <td>0.039147</td>\n",
       "      <td>0.028222</td>\n",
       "      <td>0.028222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.564745</td>\n",
       "      <td>15.128031</td>\n",
       "      <td>0.517650</td>\n",
       "      <td>7.805053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  len_location  len_keyword  len_text  len_text_original  \\\n",
       "0         0             0            0        23                 34   \n",
       "1         2             0            0        45                 64   \n",
       "2         3             0            0        53                 96   \n",
       "3         9             0            0        30                 40   \n",
       "4        11             0            0        40                 45   \n",
       "...     ...           ...          ...       ...                ...   \n",
       "3258  10861             0            0        44                 55   \n",
       "3259  10865             0            0       114                139   \n",
       "3260  10868             0            0        30                 55   \n",
       "3261  10874             0            0        40                 65   \n",
       "3262  10875             0            0        46                 68   \n",
       "\n",
       "      diff_len_text  keyword_value_sin_stemming  keyword_value_con_stemming  \\\n",
       "0                11                    3.267254                     0.03008   \n",
       "1                19                    3.267254                     0.03008   \n",
       "2                43                    3.267254                     0.03008   \n",
       "3                10                    3.267254                     0.03008   \n",
       "4                 5                    3.267254                     0.03008   \n",
       "...             ...                         ...                         ...   \n",
       "3258             11                    3.267254                     0.03008   \n",
       "3259             25                    3.267254                     0.03008   \n",
       "3260             25                    3.267254                     0.03008   \n",
       "3261             25                    3.267254                     0.03008   \n",
       "3262             22                    3.267254                     0.03008   \n",
       "\n",
       "      keyword_value_mult  keyword_value_diff  ...  text_value_2_gram  \\\n",
       "0               0.098278            3.237175  ...           0.027890   \n",
       "1               0.098278            3.237175  ...           0.060072   \n",
       "2               0.098278            3.237175  ...           0.054311   \n",
       "3               0.098278            3.237175  ...           0.000000   \n",
       "4               0.098278            3.237175  ...           0.086690   \n",
       "...                  ...                 ...  ...                ...   \n",
       "3258            0.098278            3.237175  ...           0.026343   \n",
       "3259            0.098278            3.237175  ...           0.142852   \n",
       "3260            0.098278            3.237175  ...           0.059477   \n",
       "3261            0.098278            3.237175  ...           0.141421   \n",
       "3262            0.098278            3.237175  ...           0.057778   \n",
       "\n",
       "      text_norm_value_2_gram  text_value_3_gram  text_norm_value_3_gram  \\\n",
       "0                   0.027890           0.000000                0.000000   \n",
       "1                   0.046347           0.027337                0.027337   \n",
       "2                   0.037508           0.000000                0.000000   \n",
       "3                   0.000000           0.000000                0.000000   \n",
       "4                   0.052940           0.000000                0.000000   \n",
       "...                      ...                ...                     ...   \n",
       "3258                0.026343           0.000000                0.000000   \n",
       "3259                0.066315           0.030143                0.030143   \n",
       "3260                0.039728           0.030895                0.030895   \n",
       "3261                0.070943           0.112221                0.059870   \n",
       "3262                0.039147           0.028222                0.028222   \n",
       "\n",
       "      text_value_4_gram  text_norm_value_4_gram  text_value_mult  \\\n",
       "0              0.000000                0.000000         1.305677   \n",
       "1              0.000000                0.000000         4.531304   \n",
       "2              0.000000                0.000000        10.661704   \n",
       "3              0.000000                0.000000         0.961869   \n",
       "4              0.000000                0.000000         2.802071   \n",
       "...                 ...                     ...              ...   \n",
       "3258           0.000000                0.000000         2.791938   \n",
       "3259           0.000000                0.000000        33.175360   \n",
       "3260           0.000000                0.000000         2.446438   \n",
       "3261           0.086311                0.045653         4.107835   \n",
       "3262           0.000000                0.000000         2.564745   \n",
       "\n",
       "      text_value_diff  text_norm_value_mult  text_norm_value_diff  \n",
       "0           11.145168              0.433205              7.178612  \n",
       "1           23.119374              0.751209             10.944429  \n",
       "2           45.098771              1.461376             17.442445  \n",
       "3           11.282421              0.328020              7.421708  \n",
       "4           15.976074              0.593750              8.324345  \n",
       "...               ...                   ...                   ...  \n",
       "3258        19.243501              0.832196             10.592598  \n",
       "3259        59.897514              2.622387             23.467875  \n",
       "3260        16.745660              0.504244              8.611057  \n",
       "3261        20.084189              0.686354              9.431351  \n",
       "3262        15.128031              0.517650              7.805053  \n",
       "\n",
       "[3263 rows x 55 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded.to_csv('test/test_encoded.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
