{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ben/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ben/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean, median\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import category_encoders as ce #pip install category_encoders\n",
    "import nltk #pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encoder(col_name, col):\n",
    "    \n",
    "    ce_bin = ce.BinaryEncoder(cols = [col_name], drop_invariant=True)\n",
    "    \n",
    "    return ce_bin.fit_transform(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_encoder(col_text):\n",
    "    \n",
    "    tf_idf_vec = TfidfVectorizer(use_idf=True, smooth_idf=False, ngram_range=(1,1), tokenizer=nltk.word_tokenize)\n",
    "    tf_idf_data = tf_idf_vec.fit_transform(col_text)\n",
    "    tf_idf_array = tf_idf_data.toarray()\n",
    "    vocabulary = tf_idf_vec.vocabulary_\n",
    "    \n",
    "    return tf_idf_array, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_values(col, tf_idf_array, vocabulary):\n",
    "\n",
    "    len_text = len(col)\n",
    "    sum_values = []; min_values = []; max_values = []; mean_values = []; median_values = []\n",
    "    \n",
    "    for i in range(len_text):\n",
    "        values = []\n",
    "        for word in col.iloc[i].split():\n",
    "            if word in vocabulary:\n",
    "                pos_word = vocabulary[word]\n",
    "                values.append(tf_idf_array[i][pos_word])\n",
    "            \n",
    "        if values:\n",
    "            sum_values.append(sum(values))\n",
    "            min_values.append(min(values))\n",
    "            max_values.append(max(values))\n",
    "            mean_values.append(mean(values))\n",
    "            median_values.append(median(values))\n",
    "        else:\n",
    "            sum_values.append(0)\n",
    "            min_values.append(0)\n",
    "            max_values.append(0)\n",
    "            mean_values.append(0)\n",
    "            median_values.append(0)\n",
    "            \n",
    "    column_values = {'sum': sum_values, 'min': min_values, 'max': max_values,\\\n",
    "                     'mean': mean_values, 'median': median_values}\n",
    "    return column_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_google_news = gensim_api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = pd.Series([['hola que tal', 'como esta'], ['no sé qué decir', 'jejeje']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = aux.iloc[0][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = {'Hola que': 2, 'tal como': 3}\n",
    "\n",
    "aux = \"Hola que tal como va\"\n",
    "n_grams = ngrams(aux.split(), 1)\n",
    "c = []\n",
    "for grams in n_grams:\n",
    "    c.append([' '.join(grams)])\n",
    "    \n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(min_count=1)\n",
    "model.build_vocab(c)\n",
    "model.train(c, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [a, ['no', 'sé', 'qué', 'decir', 'jejeje', 'no']]\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_model(list_of_words):\n",
    "    model = word2vec.Word2Vec(min_count=2)\n",
    "    model.build_vocab(list_of_words)\n",
    "    model.train(list_of_words, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(x, y):\n",
    "    return np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_values_w2v(col, w2v):\n",
    "    \n",
    "    len_col = len(col)\n",
    "    sum_values = []; min_values = []; max_values = []; mean_values = []; median_values = []\n",
    "    \n",
    "    for i in range(len_col):\n",
    "        words = col.iloc[i].split()\n",
    "        array_values = []\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                array_values.append(w2v[word])\n",
    "        values = []\n",
    "        len_array_values = len(array_values)\n",
    "        for j in range(len_array_values):\n",
    "            for k in range(j+1, len_array_values):\n",
    "                values.append(cos(array_values[j], array_values[k]))\n",
    "                \n",
    "        if values:\n",
    "            sum_values.append(sum(values))\n",
    "            min_values.append(min(values))\n",
    "            max_values.append(max(values))\n",
    "            mean_values.append(mean(values))\n",
    "            median_values.append(median(values))\n",
    "        else:\n",
    "            sum_values.append(0)\n",
    "            min_values.append(0)\n",
    "            max_values.append(0)\n",
    "            mean_values.append(0)\n",
    "            median_values.append(0)\n",
    "            \n",
    "    column_values = {'sum': sum_values, 'min': min_values, 'max': max_values,\\\n",
    "                     'mean': mean_values, 'median': median_values}\n",
    "    return column_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_of_norms(col, w2v, num_n_gram=1):\n",
    "    \n",
    "    len_col = len(col)\n",
    "    sum_values = [];\n",
    "    \n",
    "    for i in range(len_col):\n",
    "        words = []\n",
    "        n_grams = ngrams(col.iloc[i].split(), num_n_gram)\n",
    "        for grams in n_grams:\n",
    "            words.append(' '.join(grams))\n",
    "#        words = col.iloc[i].split()\n",
    "        array_values = []\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                array_values.append(w2v[word])\n",
    "        values = []\n",
    "        len_array_values = len(array_values)\n",
    "        for j in range(len_array_values):\n",
    "            values.append(np.linalg.norm(array_values[j]))\n",
    "                \n",
    "        if values:\n",
    "            sum_values.append(sum(values))\n",
    "        else:\n",
    "            sum_values.append(0)\n",
    "            \n",
    "    column_values = {'sum': sum_values}\n",
    "    return column_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_norm_of_sum(col, w2v, num_n_gram=1):\n",
    "    \n",
    "    len_col = len(col)\n",
    "    norms = [];\n",
    "    \n",
    "    for i in range(len_col):\n",
    "        words = []\n",
    "        n_grams = ngrams(col.iloc[i].split(), num_n_gram)\n",
    "        for grams in n_grams:\n",
    "            words.append(' '.join(grams))\n",
    "#        words = col.iloc[i].split()\n",
    "        array_values = []\n",
    "        for word in words:\n",
    "            if word in w2v:\n",
    "                array_values.append(w2v[word])\n",
    "                \n",
    "        norms.append(np.linalg.norm(np.sum(array_values, axis=0)))\n",
    "                \n",
    "    return norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_len(col):\n",
    "    \n",
    "    len_string = []\n",
    "    len_col = len(col)\n",
    "    for i in range(len_col):\n",
    "        if col.iloc[i] is not np.nan:\n",
    "            len_string.append(len(col.iloc[i]))\n",
    "        else:\n",
    "            len_string.append(0)\n",
    "        \n",
    "    return len_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram_list(col, num_n_gram=1):\n",
    "    len_col = len(col)\n",
    "    words = []\n",
    "    for i in range(len_col):    \n",
    "        n_grams = ngrams(col.iloc[i].split(), num_n_gram)\n",
    "        for grams in n_grams:\n",
    "            words.append([' '.join(grams)])\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train/train_limpio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(train['keyword_sin_stemming'].astype('str'), model_google_news)\n",
    "\n",
    "train['keyword_value'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(train['keyword_con_stemming'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(train['keyword_con_stemming'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "train['keyword_sum'] = [value for value in column_values['sum']]\n",
    "train['keyword_min'] = [value for value in column_values['min']]\n",
    "train['keyword_max'] = [value for value in column_values['max']]\n",
    "train['keyword_mean'] = [value for value in column_values['mean']]\n",
    "train['keyword_median'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_keyword = calculate_len(train['keyword_con_stemming'])\n",
    "\n",
    "pos_col_keyword = train.columns.get_loc('keyword_con_stemming')+1\n",
    "train.insert(loc=pos_col_keyword, column='len_keyword', value=len_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-488b9d39536e>:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-488b9d39536e>:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    location_as_list = get_n_gram_list(train['location'].astype('str'), i)\n",
    "    model_location = get_w2v_model(location_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(train['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(train['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_norm_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_as_list = get_n_gram_list(train['location'].astype('str'), 1)\n",
    "model_location = get_w2v_model(location_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-4793a7e59de5>:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-8-4793a7e59de5>:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "column_values = calculate_values_w2v(train['location'].astype('str'), model_location)\n",
    "\n",
    "train['location_sum_w2v'] = [value for value in column_values['sum']]\n",
    "train['location_min_w2v'] = [value for value in column_values['min']]\n",
    "train['location_max_w2v'] = [value for value in column_values['max']]\n",
    "train['location_mean_w2v'] = [value for value in column_values['mean']]\n",
    "train['location_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(train['location'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(train['location'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "train['location_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "train['location_min_tf-idf'] = [value for value in column_values['min']]\n",
    "train['location_max_tf-idf'] = [value for value in column_values['max']]\n",
    "train['location_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "train['location_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_location = calculate_len(train['location'])\n",
    "\n",
    "pos_col_location = train.columns.get_loc('location')+1\n",
    "train.insert(loc=pos_col_location, column='len_location', value=len_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(train['text_con_stemming'])\n",
    "\n",
    "column_values = calculate_values(train['text_con_stemming'], tf_idf_array, vocabulary)\n",
    "\n",
    "train['text_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "train['text_min_tf-idf'] = [value for value in column_values['min']]\n",
    "train['text_max_tf-idf'] = [value for value in column_values['max']]\n",
    "train['text_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "train['text_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(train['text_sin_stemming'], model_google_news)\n",
    "\n",
    "train['text_value'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_norm_of_sum(train['text_sin_stemming'], model_google_news)\n",
    "\n",
    "train['text_norm_value'] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_values_w2v(train['text_sin_stemming'], model_google_news)\n",
    "\n",
    "train['text_sum_w2v'] = [value for value in column_values['sum']]\n",
    "train['text_min_w2v'] = [value for value in column_values['min']]\n",
    "train['text_max_w2v'] = [value for value in column_values['max']]\n",
    "train['text_mean_w2v'] = [value for value in column_values['mean']]\n",
    "train['text_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-488b9d39536e>:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-488b9d39536e>:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    text_as_list = get_n_gram_list(train['text_con_stemming'].astype('str'), i)\n",
    "    model_text = get_w2v_model(text_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(train['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(train['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_norm_value_\" + str(i) + \"_gram\"\n",
    "    train[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "cols.append('target')\n",
    "train = train[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>len_location</th>\n",
       "      <th>keyword_sin_stemming</th>\n",
       "      <th>keyword_con_stemming</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>text_con_stemming</th>\n",
       "      <th>len_text</th>\n",
       "      <th>text_sin_stemming</th>\n",
       "      <th>keyword_value</th>\n",
       "      <th>keyword_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>text_median_w2v</th>\n",
       "      <th>text_value_1_gram</th>\n",
       "      <th>text_norm_value_1_gram</th>\n",
       "      <th>text_value_2_gram</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>deed reason earthquak may alah forgiv us al</td>\n",
       "      <td>43</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121727</td>\n",
       "      <td>0.227191</td>\n",
       "      <td>0.077967</td>\n",
       "      <td>0.056394</td>\n",
       "      <td>0.040931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>36</td>\n",
       "      <td>forest fire near la ronge sask  canada</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055061</td>\n",
       "      <td>0.143954</td>\n",
       "      <td>0.061261</td>\n",
       "      <td>0.056181</td>\n",
       "      <td>0.042664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>al resid ask shelter place notifi offic evacu ...</td>\n",
       "      <td>72</td>\n",
       "      <td>al residents asked to shelter in place are bei...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095317</td>\n",
       "      <td>0.313103</td>\n",
       "      <td>0.117570</td>\n",
       "      <td>0.055537</td>\n",
       "      <td>0.055537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>number peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>50</td>\n",
       "      <td>number  people receive wildfires evacuation o...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082558</td>\n",
       "      <td>0.197538</td>\n",
       "      <td>0.078924</td>\n",
       "      <td>0.059248</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>52</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086994</td>\n",
       "      <td>0.265268</td>\n",
       "      <td>0.083965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>rockyfir updat california hwi number close dir...</td>\n",
       "      <td>84</td>\n",
       "      <td>rockyfire update  california hwy   number  clo...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080587</td>\n",
       "      <td>0.375873</td>\n",
       "      <td>0.104406</td>\n",
       "      <td>0.087474</td>\n",
       "      <td>0.056522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>flood disast heavi rain caus flash flood stree...</td>\n",
       "      <td>74</td>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109943</td>\n",
       "      <td>0.318629</td>\n",
       "      <td>0.104069</td>\n",
       "      <td>0.115915</td>\n",
       "      <td>0.060897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>top hil see fire wood</td>\n",
       "      <td>21</td>\n",
       "      <td>i am on top of the hil and i can see a fire in...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112546</td>\n",
       "      <td>0.140337</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>0.028337</td>\n",
       "      <td>0.028337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>emerg evacu happen build across street</td>\n",
       "      <td>38</td>\n",
       "      <td>there is an emergency evacuation happening now...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158737</td>\n",
       "      <td>0.171633</td>\n",
       "      <td>0.073330</td>\n",
       "      <td>0.029165</td>\n",
       "      <td>0.029165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>afraid tornado come area</td>\n",
       "      <td>24</td>\n",
       "      <td>i am afraid that the tornado is coming to our ...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136866</td>\n",
       "      <td>0.115179</td>\n",
       "      <td>0.054023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  location  len_location keyword_sin_stemming keyword_con_stemming  \\\n",
       "0      NaN             0                  NaN                  NaN   \n",
       "1      NaN             0                  NaN                  NaN   \n",
       "2      NaN             0                  NaN                  NaN   \n",
       "3      NaN             0                  NaN                  NaN   \n",
       "4      NaN             0                  NaN                  NaN   \n",
       "5      NaN             0                  NaN                  NaN   \n",
       "6      NaN             0                  NaN                  NaN   \n",
       "7      NaN             0                  NaN                  NaN   \n",
       "8      NaN             0                  NaN                  NaN   \n",
       "9      NaN             0                  NaN                  NaN   \n",
       "\n",
       "   len_keyword                                  text_con_stemming  len_text  \\\n",
       "0            0        deed reason earthquak may alah forgiv us al        43   \n",
       "1            0               forest fire near la rong sask canada        36   \n",
       "2            0  al resid ask shelter place notifi offic evacu ...        72   \n",
       "3            0  number peopl receiv wildfir evacu order califo...        50   \n",
       "4            0  got sent photo rubi alaska smoke wildfir pour ...        52   \n",
       "5            0  rockyfir updat california hwi number close dir...        84   \n",
       "6            0  flood disast heavi rain caus flash flood stree...        74   \n",
       "7            0                              top hil see fire wood        21   \n",
       "8            0             emerg evacu happen build across street        38   \n",
       "9            0                           afraid tornado come area        24   \n",
       "\n",
       "                                   text_sin_stemming  keyword_value  \\\n",
       "0  our deeds are the reason of this earthquake ma...       3.267254   \n",
       "1             forest fire near la ronge sask  canada       3.267254   \n",
       "2  al residents asked to shelter in place are bei...       3.267254   \n",
       "3   number  people receive wildfires evacuation o...       3.267254   \n",
       "4  just got sent this photo from ruby alaska as s...       3.267254   \n",
       "5  rockyfire update  california hwy   number  clo...       3.267254   \n",
       "6  flood disaster heavy rain causes flash floodin...       3.267254   \n",
       "7  i am on top of the hil and i can see a fire in...       3.267254   \n",
       "8  there is an emergency evacuation happening now...       3.267254   \n",
       "9  i am afraid that the tornado is coming to our ...       3.267254   \n",
       "\n",
       "   keyword_sum  ...  text_median_w2v  text_value_1_gram  \\\n",
       "0          1.0  ...         0.121727           0.227191   \n",
       "1          1.0  ...         0.055061           0.143954   \n",
       "2          1.0  ...         0.095317           0.313103   \n",
       "3          1.0  ...         0.082558           0.197538   \n",
       "4          1.0  ...         0.086994           0.265268   \n",
       "5          1.0  ...         0.080587           0.375873   \n",
       "6          1.0  ...         0.109943           0.318629   \n",
       "7          1.0  ...         0.112546           0.140337   \n",
       "8          1.0  ...         0.158737           0.171633   \n",
       "9          1.0  ...         0.136866           0.115179   \n",
       "\n",
       "   text_norm_value_1_gram  text_value_2_gram  text_norm_value_2_gram  \\\n",
       "0                0.077967           0.056394                0.040931   \n",
       "1                0.061261           0.056181                0.042664   \n",
       "2                0.117570           0.055537                0.055537   \n",
       "3                0.078924           0.059248                0.043484   \n",
       "4                0.083965           0.000000                0.000000   \n",
       "5                0.104406           0.087474                0.056522   \n",
       "6                0.104069           0.115915                0.060897   \n",
       "7                0.058818           0.028337                0.028337   \n",
       "8                0.073330           0.029165                0.029165   \n",
       "9                0.054023           0.000000                0.000000   \n",
       "\n",
       "   text_value_3_gram  text_norm_value_3_gram  text_value_4_gram  \\\n",
       "0                0.0                     0.0                0.0   \n",
       "1                0.0                     0.0                0.0   \n",
       "2                0.0                     0.0                0.0   \n",
       "3                0.0                     0.0                0.0   \n",
       "4                0.0                     0.0                0.0   \n",
       "5                0.0                     0.0                0.0   \n",
       "6                0.0                     0.0                0.0   \n",
       "7                0.0                     0.0                0.0   \n",
       "8                0.0                     0.0                0.0   \n",
       "9                0.0                     0.0                0.0   \n",
       "\n",
       "   text_norm_value_4_gram  target  \n",
       "0                     0.0       1  \n",
       "1                     0.0       1  \n",
       "2                     0.0       1  \n",
       "3                     0.0       1  \n",
       "4                     0.0       1  \n",
       "5                     0.0       1  \n",
       "6                     0.0       1  \n",
       "7                     0.0       1  \n",
       "8                     0.0       1  \n",
       "9                     0.0       1  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(train['text_con_stemming'])\n",
    "\n",
    "pos_col_text = train.columns.get_loc('text_con_stemming')+1\n",
    "train.insert(loc=pos_col_text, column='len_text', value=len_text)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_original = pd.read_csv('train/train_original.csv')\n",
    "train_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>len_text_original</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8  14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "\n",
       "   len_text_original  target  \n",
       "0                 69       1  \n",
       "1                 38       1  \n",
       "2                133       1  \n",
       "3                 65       1  \n",
       "4                 88       1  \n",
       "5                110       1  \n",
       "6                 95       1  \n",
       "7                 59       1  \n",
       "8                 79       1  \n",
       "9                 52       1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(train_original['text'])\n",
    "\n",
    "pos_col_text = train_original.columns.get_loc('text')+1\n",
    "train_original.insert(loc=pos_col_text, column='len_text_original', value=len_text)\n",
    "train_original.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns.tolist()\n",
    "cols.remove('location')\n",
    "cols.remove('text_con_stemming')\n",
    "cols.remove('text_sin_stemming')\n",
    "cols.remove('keyword_con_stemming')\n",
    "cols.remove('keyword_sin_stemming')\n",
    "cols.remove('target')\n",
    "cols.append('target')\n",
    "train = train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = train[cols]\n",
    "\n",
    "pos_col_text = train_encoded.columns.get_loc('len_text')+1\n",
    "train_encoded.insert(loc=pos_col_text, column='len_text_original', value=train_original['len_text_original'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.insert(loc=pos_col_text+1, column='diff_len_text',\\\n",
    "                     value=train_encoded['len_text_original']-train_encoded['len_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_location</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_text_original</th>\n",
       "      <th>diff_len_text</th>\n",
       "      <th>keyword_value</th>\n",
       "      <th>keyword_sum</th>\n",
       "      <th>keyword_min</th>\n",
       "      <th>keyword_max</th>\n",
       "      <th>keyword_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>text_median_w2v</th>\n",
       "      <th>text_value_1_gram</th>\n",
       "      <th>text_norm_value_1_gram</th>\n",
       "      <th>text_value_2_gram</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>69</td>\n",
       "      <td>26</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121727</td>\n",
       "      <td>0.227191</td>\n",
       "      <td>0.077967</td>\n",
       "      <td>0.056394</td>\n",
       "      <td>0.040931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055061</td>\n",
       "      <td>0.143954</td>\n",
       "      <td>0.061261</td>\n",
       "      <td>0.056181</td>\n",
       "      <td>0.042664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>133</td>\n",
       "      <td>61</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095317</td>\n",
       "      <td>0.313103</td>\n",
       "      <td>0.117570</td>\n",
       "      <td>0.055537</td>\n",
       "      <td>0.055537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082558</td>\n",
       "      <td>0.197538</td>\n",
       "      <td>0.078924</td>\n",
       "      <td>0.059248</td>\n",
       "      <td>0.043484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>88</td>\n",
       "      <td>36</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086994</td>\n",
       "      <td>0.265268</td>\n",
       "      <td>0.083965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>83</td>\n",
       "      <td>33</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090719</td>\n",
       "      <td>0.252617</td>\n",
       "      <td>0.087472</td>\n",
       "      <td>0.230477</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>0.200416</td>\n",
       "      <td>0.077877</td>\n",
       "      <td>0.179269</td>\n",
       "      <td>0.062804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>125</td>\n",
       "      <td>43</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158652</td>\n",
       "      <td>0.317482</td>\n",
       "      <td>0.090036</td>\n",
       "      <td>0.290268</td>\n",
       "      <td>0.092334</td>\n",
       "      <td>0.254335</td>\n",
       "      <td>0.087856</td>\n",
       "      <td>0.226830</td>\n",
       "      <td>0.074986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046951</td>\n",
       "      <td>0.290424</td>\n",
       "      <td>0.150312</td>\n",
       "      <td>0.259849</td>\n",
       "      <td>0.107001</td>\n",
       "      <td>0.231129</td>\n",
       "      <td>0.087631</td>\n",
       "      <td>0.200353</td>\n",
       "      <td>0.075999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>137</td>\n",
       "      <td>41</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085732</td>\n",
       "      <td>0.489610</td>\n",
       "      <td>0.136034</td>\n",
       "      <td>0.463238</td>\n",
       "      <td>0.137259</td>\n",
       "      <td>0.440136</td>\n",
       "      <td>0.121416</td>\n",
       "      <td>0.403074</td>\n",
       "      <td>0.118529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>94</td>\n",
       "      <td>36</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084648</td>\n",
       "      <td>0.254336</td>\n",
       "      <td>0.089005</td>\n",
       "      <td>0.233998</td>\n",
       "      <td>0.083966</td>\n",
       "      <td>0.205074</td>\n",
       "      <td>0.074552</td>\n",
       "      <td>0.171774</td>\n",
       "      <td>0.070728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      len_location  len_keyword  len_text  len_text_original  diff_len_text  \\\n",
       "0                0            0        43                 69             26   \n",
       "1                0            0        36                 38              2   \n",
       "2                0            0        72                133             61   \n",
       "3                0            0        50                 65             15   \n",
       "4                0            0        52                 88             36   \n",
       "...            ...          ...       ...                ...            ...   \n",
       "7608             0            0        50                 83             33   \n",
       "7609             0            0        82                125             43   \n",
       "7610             0            0        60                 65              5   \n",
       "7611             0            0        96                137             41   \n",
       "7612             0            0        58                 94             36   \n",
       "\n",
       "      keyword_value  keyword_sum  keyword_min  keyword_max  keyword_mean  ...  \\\n",
       "0          3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "1          3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "2          3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "3          3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "4          3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "...             ...          ...          ...          ...           ...  ...   \n",
       "7608       3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "7609       3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "7610       3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "7611       3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "7612       3.267254          1.0          1.0          1.0           1.0  ...   \n",
       "\n",
       "      text_median_w2v  text_value_1_gram  text_norm_value_1_gram  \\\n",
       "0            0.121727           0.227191                0.077967   \n",
       "1            0.055061           0.143954                0.061261   \n",
       "2            0.095317           0.313103                0.117570   \n",
       "3            0.082558           0.197538                0.078924   \n",
       "4            0.086994           0.265268                0.083965   \n",
       "...               ...                ...                     ...   \n",
       "7608         0.090719           0.252617                0.087472   \n",
       "7609         0.158652           0.317482                0.090036   \n",
       "7610         0.046951           0.290424                0.150312   \n",
       "7611         0.085732           0.489610                0.136034   \n",
       "7612         0.084648           0.254336                0.089005   \n",
       "\n",
       "      text_value_2_gram  text_norm_value_2_gram  text_value_3_gram  \\\n",
       "0              0.056394                0.040931           0.000000   \n",
       "1              0.056181                0.042664           0.000000   \n",
       "2              0.055537                0.055537           0.000000   \n",
       "3              0.059248                0.043484           0.000000   \n",
       "4              0.000000                0.000000           0.000000   \n",
       "...                 ...                     ...                ...   \n",
       "7608           0.230477                0.095556           0.200416   \n",
       "7609           0.290268                0.092334           0.254335   \n",
       "7610           0.259849                0.107001           0.231129   \n",
       "7611           0.463238                0.137259           0.440136   \n",
       "7612           0.233998                0.083966           0.205074   \n",
       "\n",
       "      text_norm_value_3_gram  text_value_4_gram  text_norm_value_4_gram  \\\n",
       "0                   0.000000           0.000000                0.000000   \n",
       "1                   0.000000           0.000000                0.000000   \n",
       "2                   0.000000           0.000000                0.000000   \n",
       "3                   0.000000           0.000000                0.000000   \n",
       "4                   0.000000           0.000000                0.000000   \n",
       "...                      ...                ...                     ...   \n",
       "7608                0.077877           0.179269                0.062804   \n",
       "7609                0.087856           0.226830                0.074986   \n",
       "7610                0.087631           0.200353                0.075999   \n",
       "7611                0.121416           0.403074                0.118529   \n",
       "7612                0.074552           0.171774                0.070728   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 48 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded.to_csv('train/train_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test/test_limpio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(test['keyword_sin_stemming'].astype('str'), model_google_news)\n",
    "test['keyword_value'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(test['keyword_con_stemming'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(test['keyword_con_stemming'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "test['keyword_sum'] = [value for value in column_values['sum']]\n",
    "test['keyword_min'] = [value for value in column_values['min']]\n",
    "test['keyword_max'] = [value for value in column_values['max']]\n",
    "test['keyword_mean'] = [value for value in column_values['mean']]\n",
    "test['keyword_median'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_keyword = calculate_len(test['keyword_con_stemming'])\n",
    "\n",
    "pos_col_keyword = test.columns.get_loc('keyword_con_stemming')+1\n",
    "test.insert(loc=pos_col_keyword, column='len_keyword', value=len_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-488b9d39536e>:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-488b9d39536e>:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    location_as_list = get_n_gram_list(test['location'].astype('str'), i)\n",
    "    model_location = get_w2v_model(location_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(test['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(test['location'].astype('str'), model_location, i)\n",
    "    \n",
    "    column_name = \"location_norm_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-4793a7e59de5>:10: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n"
     ]
    }
   ],
   "source": [
    "column_values = calculate_values_w2v(test['location'].astype('str'), model_location)\n",
    "\n",
    "test['location_sum_w2v'] = [value for value in column_values['sum']]\n",
    "test['location_min_w2v'] = [value for value in column_values['min']]\n",
    "test['location_max_w2v'] = [value for value in column_values['max']]\n",
    "test['location_mean_w2v'] = [value for value in column_values['mean']]\n",
    "test['location_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(test['location'].astype('str'))\n",
    "\n",
    "column_values = calculate_values(test['location'].astype('str'), tf_idf_array, vocabulary)\n",
    "\n",
    "test['location_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "test['location_min_tf-idf'] = [value for value in column_values['min']]\n",
    "test['location_max_tf-idf'] = [value for value in column_values['max']]\n",
    "test['location_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "test['location_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_location = calculate_len(test['location'])\n",
    "\n",
    "pos_col_location = test.columns.get_loc('location')+1\n",
    "test.insert(loc=pos_col_location, column='len_location', value=len_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text_con_stemming'] = test['text_con_stemming'].astype('str')\n",
    "test['text_sin_stemming'] = test['text_sin_stemming'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array, vocabulary = tf_idf_encoder(test['text_con_stemming'])\n",
    "\n",
    "column_values = calculate_values(test['text_con_stemming'], tf_idf_array, vocabulary)\n",
    "\n",
    "test['text_sum_tf-idf'] = [value for value in column_values['sum']]\n",
    "test['text_min_tf-idf'] = [value for value in column_values['min']]\n",
    "test['text_max_tf-idf'] = [value for value in column_values['max']]\n",
    "test['text_mean_tf-idf'] = [value for value in column_values['mean']]\n",
    "test['text_median_tf-idf'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_sum_of_norms(test['text_sin_stemming'], model_google_news)\n",
    "\n",
    "test['text_value'] = [value for value in column_values['sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_norm_of_sum(test['text_sin_stemming'], model_google_news)\n",
    "\n",
    "test['text_norm_value'] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = calculate_values_w2v(test['text_sin_stemming'], model_google_news)\n",
    "\n",
    "test['text_sum_w2v'] = [value for value in column_values['sum']]\n",
    "test['text_min_w2v'] = [value for value in column_values['min']]\n",
    "test['text_max_w2v'] = [value for value in column_values['max']]\n",
    "test['text_mean_w2v'] = [value for value in column_values['mean']]\n",
    "test['text_median_w2v'] = [value for value in column_values['median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-488b9d39536e>:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if word in w2v:\n",
      "<ipython-input-9-488b9d39536e>:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  array_values.append(w2v[word])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    text_as_list = get_n_gram_list(test['text_con_stemming'].astype('str'), i)\n",
    "    model_text = get_w2v_model(text_as_list)\n",
    "    \n",
    "    column_values = calculate_sum_of_norms(test['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values['sum']]\n",
    "    \n",
    "    column_values = calculate_norm_of_sum(test['text_con_stemming'].astype('str'), model_text, i)\n",
    "    \n",
    "    column_name = \"text_norm_value_\" + str(i) + \"_gram\"\n",
    "    test[column_name] = [value for value in column_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>len_location</th>\n",
       "      <th>keyword_sin_stemming</th>\n",
       "      <th>keyword_con_stemming</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>text_con_stemming</th>\n",
       "      <th>len_text</th>\n",
       "      <th>text_sin_stemming</th>\n",
       "      <th>keyword_value</th>\n",
       "      <th>...</th>\n",
       "      <th>text_mean_w2v</th>\n",
       "      <th>text_median_w2v</th>\n",
       "      <th>text_value_1_gram</th>\n",
       "      <th>text_norm_value_1_gram</th>\n",
       "      <th>text_value_2_gram</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>happen teribl car crash</td>\n",
       "      <td>23</td>\n",
       "      <td>just happened a terible car crash</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233879</td>\n",
       "      <td>0.217761</td>\n",
       "      <td>0.114147</td>\n",
       "      <td>0.049523</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "      <td>45</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127819</td>\n",
       "      <td>0.103810</td>\n",
       "      <td>0.211091</td>\n",
       "      <td>0.084403</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.032063</td>\n",
       "      <td>0.028167</td>\n",
       "      <td>0.028167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "      <td>53</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106425</td>\n",
       "      <td>0.095932</td>\n",
       "      <td>0.225314</td>\n",
       "      <td>0.068850</td>\n",
       "      <td>0.054941</td>\n",
       "      <td>0.042697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "      <td>30</td>\n",
       "      <td>apocalypse lighting  spokane wildfires</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130179</td>\n",
       "      <td>0.148877</td>\n",
       "      <td>0.085165</td>\n",
       "      <td>0.051382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>typhoon soudelor kil number china taiwan</td>\n",
       "      <td>40</td>\n",
       "      <td>typhoon soudelor kils  number  in china and ta...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071147</td>\n",
       "      <td>0.041079</td>\n",
       "      <td>0.170321</td>\n",
       "      <td>0.064146</td>\n",
       "      <td>0.084029</td>\n",
       "      <td>0.046680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>shake earthquak</td>\n",
       "      <td>15</td>\n",
       "      <td>we are shaking   it is an earthquake</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155288</td>\n",
       "      <td>0.093425</td>\n",
       "      <td>0.028448</td>\n",
       "      <td>0.028448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>would probabl stil show life arsen yesterday e...</td>\n",
       "      <td>50</td>\n",
       "      <td>they would probably stil show more life than a...</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.137811</td>\n",
       "      <td>0.258151</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.027384</td>\n",
       "      <td>0.027384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>hey</td>\n",
       "      <td>3</td>\n",
       "      <td>hey how are you</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257048</td>\n",
       "      <td>0.215696</td>\n",
       "      <td>0.028073</td>\n",
       "      <td>0.028073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>nice hat</td>\n",
       "      <td>8</td>\n",
       "      <td>what a nice hat</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183303</td>\n",
       "      <td>0.189516</td>\n",
       "      <td>0.057468</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>fuck</td>\n",
       "      <td>4</td>\n",
       "      <td>fuck off</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164787</td>\n",
       "      <td>0.164787</td>\n",
       "      <td>0.028153</td>\n",
       "      <td>0.028153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id location  len_location keyword_sin_stemming keyword_con_stemming  \\\n",
       "0   0      NaN             0                  NaN                  NaN   \n",
       "1   2      NaN             0                  NaN                  NaN   \n",
       "2   3      NaN             0                  NaN                  NaN   \n",
       "3   9      NaN             0                  NaN                  NaN   \n",
       "4  11      NaN             0                  NaN                  NaN   \n",
       "5  12      NaN             0                  NaN                  NaN   \n",
       "6  21      NaN             0                  NaN                  NaN   \n",
       "7  22      NaN             0                  NaN                  NaN   \n",
       "8  27      NaN             0                  NaN                  NaN   \n",
       "9  29      NaN             0                  NaN                  NaN   \n",
       "\n",
       "   len_keyword                                  text_con_stemming  len_text  \\\n",
       "0            0                            happen teribl car crash        23   \n",
       "1            0      heard earthquak differ citi stay safe everyon        45   \n",
       "2            0  forest fire spot pond gees flee across street ...        53   \n",
       "3            0                     apocalyps light spokan wildfir        30   \n",
       "4            0           typhoon soudelor kil number china taiwan        40   \n",
       "5            0                                    shake earthquak        15   \n",
       "6            0  would probabl stil show life arsen yesterday e...        50   \n",
       "7            0                                                hey         3   \n",
       "8            0                                           nice hat         8   \n",
       "9            0                                               fuck         4   \n",
       "\n",
       "                                   text_sin_stemming  keyword_value  ...  \\\n",
       "0                  just happened a terible car crash       3.267254  ...   \n",
       "1  heard about earthquake is different cities sta...       3.267254  ...   \n",
       "2  there is a forest fire at spot pond geese are ...       3.267254  ...   \n",
       "3             apocalypse lighting  spokane wildfires       3.267254  ...   \n",
       "4  typhoon soudelor kils  number  in china and ta...       3.267254  ...   \n",
       "5               we are shaking   it is an earthquake       3.267254  ...   \n",
       "6  they would probably stil show more life than a...       3.267254  ...   \n",
       "7                                    hey how are you       3.267254  ...   \n",
       "8                                    what a nice hat       3.267254  ...   \n",
       "9                                           fuck off       3.267254  ...   \n",
       "\n",
       "   text_mean_w2v  text_median_w2v  text_value_1_gram  text_norm_value_1_gram  \\\n",
       "0       0.233879         0.217761           0.114147                0.049523   \n",
       "1       0.127819         0.103810           0.211091                0.084403   \n",
       "2       0.106425         0.095932           0.225314                0.068850   \n",
       "3       0.130179         0.148877           0.085165                0.051382   \n",
       "4       0.071147         0.041079           0.170321                0.064146   \n",
       "5       0.155288         0.093425           0.028448                0.028448   \n",
       "6       0.185300         0.137811           0.258151                0.084400   \n",
       "7       0.257048         0.215696           0.028073                0.028073   \n",
       "8       0.183303         0.189516           0.057468                0.040742   \n",
       "9       0.164787         0.164787           0.028153                0.028153   \n",
       "\n",
       "   text_value_2_gram  text_norm_value_2_gram  text_value_3_gram  \\\n",
       "0           0.027622                0.027622           0.000000   \n",
       "1           0.053241                0.032063           0.028167   \n",
       "2           0.054941                0.042697           0.000000   \n",
       "3           0.000000                0.000000           0.000000   \n",
       "4           0.084029                0.046680           0.000000   \n",
       "5           0.000000                0.000000           0.000000   \n",
       "6           0.027384                0.027384           0.000000   \n",
       "7           0.000000                0.000000           0.000000   \n",
       "8           0.000000                0.000000           0.000000   \n",
       "9           0.000000                0.000000           0.000000   \n",
       "\n",
       "   text_norm_value_3_gram  text_value_4_gram  text_norm_value_4_gram  \n",
       "0                0.000000                0.0                     0.0  \n",
       "1                0.028167                0.0                     0.0  \n",
       "2                0.000000                0.0                     0.0  \n",
       "3                0.000000                0.0                     0.0  \n",
       "4                0.000000                0.0                     0.0  \n",
       "5                0.000000                0.0                     0.0  \n",
       "6                0.000000                0.0                     0.0  \n",
       "7                0.000000                0.0                     0.0  \n",
       "8                0.000000                0.0                     0.0  \n",
       "9                0.000000                0.0                     0.0  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(test['text_con_stemming'])\n",
    "\n",
    "pos_col_text = test.columns.get_loc('text_con_stemming')+1\n",
    "test.insert(loc=pos_col_text, column='len_text', value=len_text)\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## len_text_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_original = pd.read_csv('test/test_original.csv')\n",
    "test_original.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntamos todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = test.columns.tolist()\n",
    "cols.remove('location')\n",
    "cols.remove('text_con_stemming')\n",
    "cols.remove('text_sin_stemming')\n",
    "cols.remove('keyword_con_stemming')\n",
    "cols.remove('keyword_sin_stemming')\n",
    "test_encoded = test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>len_text_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake   \n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...   \n",
       "7  22     NaN      NaN                                  Hey! How are you?   \n",
       "8  27     NaN      NaN                                   What a nice hat?   \n",
       "9  29     NaN      NaN                                          Fuck off!   \n",
       "\n",
       "   len_text_original  \n",
       "0                 34  \n",
       "1                 64  \n",
       "2                 96  \n",
       "3                 40  \n",
       "4                 45  \n",
       "5                 34  \n",
       "6                 72  \n",
       "7                 17  \n",
       "8                 16  \n",
       "9                  9  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_text = calculate_len(test_original['text'])\n",
    "\n",
    "pos_col_text = test_original.columns.get_loc('text')+1\n",
    "test_original.insert(loc=pos_col_text, column='len_text_original', value=len_text)\n",
    "test_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_col_text = test_encoded.columns.get_loc('len_text')+1\n",
    "test_encoded.insert(loc=pos_col_text, column='len_text_original', value=test_original['len_text_original'])\n",
    "\n",
    "test_encoded.insert(loc=pos_col_text+1, column='diff_len_text',\\\n",
    "                     value=test_encoded['len_text_original']-test_encoded['len_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>len_location</th>\n",
       "      <th>len_keyword</th>\n",
       "      <th>len_text</th>\n",
       "      <th>len_text_original</th>\n",
       "      <th>diff_len_text</th>\n",
       "      <th>keyword_value</th>\n",
       "      <th>keyword_sum</th>\n",
       "      <th>keyword_min</th>\n",
       "      <th>keyword_max</th>\n",
       "      <th>...</th>\n",
       "      <th>text_mean_w2v</th>\n",
       "      <th>text_median_w2v</th>\n",
       "      <th>text_value_1_gram</th>\n",
       "      <th>text_norm_value_1_gram</th>\n",
       "      <th>text_value_2_gram</th>\n",
       "      <th>text_norm_value_2_gram</th>\n",
       "      <th>text_value_3_gram</th>\n",
       "      <th>text_norm_value_3_gram</th>\n",
       "      <th>text_value_4_gram</th>\n",
       "      <th>text_norm_value_4_gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233879</td>\n",
       "      <td>0.217761</td>\n",
       "      <td>0.114147</td>\n",
       "      <td>0.049523</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.027622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>64</td>\n",
       "      <td>19</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127819</td>\n",
       "      <td>0.103810</td>\n",
       "      <td>0.211091</td>\n",
       "      <td>0.084403</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.032063</td>\n",
       "      <td>0.028167</td>\n",
       "      <td>0.028167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>96</td>\n",
       "      <td>43</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106425</td>\n",
       "      <td>0.095932</td>\n",
       "      <td>0.225314</td>\n",
       "      <td>0.068850</td>\n",
       "      <td>0.054941</td>\n",
       "      <td>0.042697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130179</td>\n",
       "      <td>0.148877</td>\n",
       "      <td>0.085165</td>\n",
       "      <td>0.051382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071147</td>\n",
       "      <td>0.041079</td>\n",
       "      <td>0.170321</td>\n",
       "      <td>0.064146</td>\n",
       "      <td>0.084029</td>\n",
       "      <td>0.046680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163409</td>\n",
       "      <td>0.117514</td>\n",
       "      <td>0.145139</td>\n",
       "      <td>0.068771</td>\n",
       "      <td>0.030796</td>\n",
       "      <td>0.030796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>139</td>\n",
       "      <td>25</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129440</td>\n",
       "      <td>0.112207</td>\n",
       "      <td>0.533441</td>\n",
       "      <td>0.120481</td>\n",
       "      <td>0.145621</td>\n",
       "      <td>0.068241</td>\n",
       "      <td>0.028989</td>\n",
       "      <td>0.028989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>55</td>\n",
       "      <td>25</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102561</td>\n",
       "      <td>0.098677</td>\n",
       "      <td>0.145668</td>\n",
       "      <td>0.061666</td>\n",
       "      <td>0.056492</td>\n",
       "      <td>0.040043</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082816</td>\n",
       "      <td>0.077447</td>\n",
       "      <td>0.207636</td>\n",
       "      <td>0.079030</td>\n",
       "      <td>0.142495</td>\n",
       "      <td>0.062806</td>\n",
       "      <td>0.116332</td>\n",
       "      <td>0.053774</td>\n",
       "      <td>0.082745</td>\n",
       "      <td>0.053332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>68</td>\n",
       "      <td>22</td>\n",
       "      <td>3.267254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110589</td>\n",
       "      <td>0.079109</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.071766</td>\n",
       "      <td>0.054358</td>\n",
       "      <td>0.041375</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  len_location  len_keyword  len_text  len_text_original  \\\n",
       "0         0             0            0        23                 34   \n",
       "1         2             0            0        45                 64   \n",
       "2         3             0            0        53                 96   \n",
       "3         9             0            0        30                 40   \n",
       "4        11             0            0        40                 45   \n",
       "...     ...           ...          ...       ...                ...   \n",
       "3258  10861             0            0        44                 55   \n",
       "3259  10865             0            0       114                139   \n",
       "3260  10868             0            0        30                 55   \n",
       "3261  10874             0            0        40                 65   \n",
       "3262  10875             0            0        46                 68   \n",
       "\n",
       "      diff_len_text  keyword_value  keyword_sum  keyword_min  keyword_max  \\\n",
       "0                11       3.267254          1.0          1.0          1.0   \n",
       "1                19       3.267254          1.0          1.0          1.0   \n",
       "2                43       3.267254          1.0          1.0          1.0   \n",
       "3                10       3.267254          1.0          1.0          1.0   \n",
       "4                 5       3.267254          1.0          1.0          1.0   \n",
       "...             ...            ...          ...          ...          ...   \n",
       "3258             11       3.267254          1.0          1.0          1.0   \n",
       "3259             25       3.267254          1.0          1.0          1.0   \n",
       "3260             25       3.267254          1.0          1.0          1.0   \n",
       "3261             25       3.267254          1.0          1.0          1.0   \n",
       "3262             22       3.267254          1.0          1.0          1.0   \n",
       "\n",
       "      ...  text_mean_w2v  text_median_w2v  text_value_1_gram  \\\n",
       "0     ...       0.233879         0.217761           0.114147   \n",
       "1     ...       0.127819         0.103810           0.211091   \n",
       "2     ...       0.106425         0.095932           0.225314   \n",
       "3     ...       0.130179         0.148877           0.085165   \n",
       "4     ...       0.071147         0.041079           0.170321   \n",
       "...   ...            ...              ...                ...   \n",
       "3258  ...       0.163409         0.117514           0.145139   \n",
       "3259  ...       0.129440         0.112207           0.533441   \n",
       "3260  ...       0.102561         0.098677           0.145668   \n",
       "3261  ...       0.082816         0.077447           0.207636   \n",
       "3262  ...       0.110589         0.079109           0.175175   \n",
       "\n",
       "      text_norm_value_1_gram  text_value_2_gram  text_norm_value_2_gram  \\\n",
       "0                   0.049523           0.027622                0.027622   \n",
       "1                   0.084403           0.053241                0.032063   \n",
       "2                   0.068850           0.054941                0.042697   \n",
       "3                   0.051382           0.000000                0.000000   \n",
       "4                   0.064146           0.084029                0.046680   \n",
       "...                      ...                ...                     ...   \n",
       "3258                0.068771           0.030796                0.030796   \n",
       "3259                0.120481           0.145621                0.068241   \n",
       "3260                0.061666           0.056492                0.040043   \n",
       "3261                0.079030           0.142495                0.062806   \n",
       "3262                0.071766           0.054358                0.041375   \n",
       "\n",
       "      text_value_3_gram  text_norm_value_3_gram  text_value_4_gram  \\\n",
       "0              0.000000                0.000000           0.000000   \n",
       "1              0.028167                0.028167           0.000000   \n",
       "2              0.000000                0.000000           0.000000   \n",
       "3              0.000000                0.000000           0.000000   \n",
       "4              0.000000                0.000000           0.000000   \n",
       "...                 ...                     ...                ...   \n",
       "3258           0.000000                0.000000           0.000000   \n",
       "3259           0.028989                0.028989           0.000000   \n",
       "3260           0.031140                0.031140           0.000000   \n",
       "3261           0.116332                0.053774           0.082745   \n",
       "3262           0.028201                0.028201           0.000000   \n",
       "\n",
       "      text_norm_value_4_gram  \n",
       "0                   0.000000  \n",
       "1                   0.000000  \n",
       "2                   0.000000  \n",
       "3                   0.000000  \n",
       "4                   0.000000  \n",
       "...                      ...  \n",
       "3258                0.000000  \n",
       "3259                0.000000  \n",
       "3260                0.000000  \n",
       "3261                0.053332  \n",
       "3262                0.000000  \n",
       "\n",
       "[3263 rows x 48 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded.to_csv('test/test_encoded.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
